{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7eadaa",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-generativeai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57508f1a",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True to search for the gemini models available\n",
    "search_for_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defition of the Rubric for the evaluation\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1.  **Completely Unacceptable Translation:**\n",
    "    *   The translation has no pertinence with the original meaning.\n",
    "    *   The generated sentence is either gibberish, makes no sense, or is completely unrelated to the archaic text.\n",
    "    *   Contains severe errors that render it incomprehensible or entirely misleading.\n",
    "\n",
    "2.  **Severe Semantic Errors/Omissions:**\n",
    "    *   The translation contains significant semantic errors, critical omissions of meaning from the archaic text, or substantial incorrect additions.\n",
    "    *   While some words might be recognizable, the core meaning is lost or heavily distorted.\n",
    "    *   The modernization is poor, leaving many archaic forms or incorrectly modernizing them.\n",
    "    *   Likely many grammatical errors in modern Italian.\n",
    "\n",
    "3.  **Partially Wrong Translation / Lackluster:**\n",
    "    *   The translation captures some of the original meaning but is lackluster or contains noticeable errors.\n",
    "    *   Errors are mostly minor (e.g., awkward phrasing, typos, minor grammatical mistakes in modern Italian, some less critical semantic misunderstandings or misinterpretations of archaic terms).\n",
    "    *   Some archaic features might be awkwardly modernized or missed.\n",
    "    *   The overall quality is mediocre; it's understandable but clearly flawed.\n",
    "\n",
    "4.  **Good Translation:**\n",
    "    *   The translation is mostly accurate and successfully conveys the core meaning of the archaic sentence.\n",
    "    *   It is substantially faithful to the original text.\n",
    "    *   The modern Italian is fluent and comprehensible.\n",
    "    *   Archaic features are generally well modernized.\n",
    "    *   There might be minor stylistic imperfections (e.g., style doesn't perfectly match natural modern Italian, slight awkwardness) or very minor errors that do not significantly impact understanding or meaning.\n",
    "\n",
    "5.  **Perfect Translation:**\n",
    "    *   The translation is completely accurate, fully conveying the meaning and nuances of the original archaic sentence.\n",
    "    *   It is perfectly fluent, natural-sounding, and grammatically correct modern Italian.\n",
    "    *   All archaic linguistic features (vocabulary, syntax, orthography) are correctly and appropriately modernized.\n",
    "    *   The style is appropriate for modern Italian.\n",
    "    *   No errors.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric Defined.\")\n",
    "\n",
    "# Formats the specific archaic-modern pair for an ongoing chat session.\n",
    "def format_individual_judgment_request(archaic_text, modern_translation):\n",
    "    return f\"\"\"\n",
    "Now, please evaluate the following pair:\n",
    "\n",
    "Original Archaic Sentence:\n",
    "\"{archaic_text}\"\n",
    "\n",
    "Candidate Modern Translation:\n",
    "\"{modern_translation}\"\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "print(\"Individual judgment request formatter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2575779",
   "metadata": {},
   "source": [
    "Config the API and the models and prompts names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List gemini models available\n",
    "if search_for_models:  # Set to True to list available models\n",
    "  import google.generativeai as genai\n",
    "\n",
    "  print(\"Available Gemini models that supports 'generateContent':\")\n",
    "  for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "      print(f\"- Name: {m.name}, Display Name: {m.display_name}, Description: {m.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ab30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell set up Gemini and prepare to loop through translation CSVs\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import configparser \n",
    "import json\n",
    "\n",
    "\n",
    "CONFIG_API_FILE_PATH = \"gemini_config.ini\" # Path to gemini configuration file\n",
    "API_KEY = None\n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "# Read API Key from gemini config file\n",
    "config = configparser.ConfigParser()\n",
    "if os.path.exists(CONFIG_API_FILE_PATH):\n",
    "    try:\n",
    "        config.read(CONFIG_API_FILE_PATH)\n",
    "        API_KEY = config.get('GEMINI', 'API_KEY', fallback=None)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read API key from {CONFIG_API_FILE_PATH}. Error: {e}\")\n",
    "else:\n",
    "    print(f\"ERROR: Configuration file {CONFIG_API_FILE_PATH} not found.\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"ERROR: Gemini API Key not found or not set correctly in gemini_config.ini.\")\n",
    "    raise ValueError(\"Gemini API Key not configured. Halting execution.\")\n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        print(\"Gemini API Key configured successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to configure Gemini API with the provided key. Error: {e}\")\n",
    "        raise ValueError(\"Gemini API Key configuration failed. Halting execution.\")\n",
    "\n",
    "JUDGE_MODEL_NAME = \"models/gemini-1.5-flash-latest\" # Gemini model selected\n",
    "\n",
    "\n",
    "# SAFETY AND GENERATION CONFIG\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    # temperature=0.1 # For more deterministic output from the judge\n",
    ")\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "# Initialize the generative model for the judge\n",
    "judge_llm_model = genai.GenerativeModel(JUDGE_MODEL_NAME) # Renamed to avoid conflict with ollama 'model' variable\n",
    "\n",
    "\n",
    "print(f\"Judge Model: {JUDGE_MODEL_NAME}\")\n",
    "print(\"Gemini Setup Complete.\")\n",
    "\n",
    "\n",
    "\n",
    "TRANSLATION_FILES_DIR = \"csvFiles/translations/\" # Directory where translation_model_prompt.csv are saved\n",
    "ARCHAIC_SENTENCE_COLUMN_IN_TRANSLATION_FILES = \"Sentence\" # Column name of archaic sentences in translation CSVs\n",
    "TRANSLATION_COLUMN_IN_TRANSLATION_FILES = \"translation\"  # Column name of the LLM's translation in translation CSVs\n",
    "\n",
    "# Models and prompt types used with Ollama to find the CSV files\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    ollama_models_used = config.get('models', [])\n",
    "    ollama_prompt_names_used = config.get('prompts', [])\n",
    "\n",
    "    print(\"Config success:\")\n",
    "    print(f\"Models: {ollama_models_used}\")\n",
    "    print(f\"Prompts: {ollama_prompt_names_used}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Config file '{CONFIG_FILE_PATH}' not found.\")\n",
    "    ollama_models_used = []\n",
    "    ollama_prompt_names_used = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: File '{CONFIG_FILE_PATH}' not a valid JSON.\")\n",
    "    ollama_models_used = []\n",
    "    ollama_prompt_names_used = []\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Will look for translation CSVs in: {TRANSLATION_FILES_DIR}\")\n",
    "print(f\"Models used: {ollama_models_used}\")\n",
    "print(f\"Using Judge LLM: {judge_llm_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d8930",
   "metadata": {},
   "source": [
    "## Judging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70992d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contain the main judging loop\n",
    "import sys\n",
    "# Initial prompt for the chat session\n",
    "initial_prompt_for_chat = f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "Your consistent task throughout this session is to assess the quality of \"Candidate Modern Translation\"s provided to you, based on their corresponding \"Original Archaic Sentence\"s.\n",
    "\n",
    "Please use the following detailed 1-5 scale and rubric for ALL your evaluations in this session:\n",
    "--- RUBRIC START ---\n",
    "{JUDGE_RUBRIC}\n",
    "--- RUBRIC END ---\n",
    "\n",
    "For each pair I provide, carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "Output ONLY the integer score for each. Do not add any explanation, prefix, or other text.\n",
    "I will provide you with sentence pairs one by one. Let's begin.\n",
    "\"\"\"\n",
    "\n",
    "# Definition of the directory for saving judged outputs\n",
    "JUDGED_OUTPUT_DIR = \"csvFiles/scores_gemini/\" \n",
    "os.makedirs(JUDGED_OUTPUT_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "\n",
    "# Loop through each combination of Ollama model and prompt type to load the corresponding CSV\n",
    "for ollama_model_name in ollama_models_used:\n",
    "    # Sanitize ollama_model_name for filename (e.g. if it's \"qwen:7B\", make it \"qwen-7B\")\n",
    "    safe_ollama_model_name = ollama_model_name.replace(\":\", \"-\").replace(\"/\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "\n",
    "    for ollama_prompt_name in ollama_prompt_names_used:\n",
    "        # This list will store judgments for the current file/combination\n",
    "        current_file_judgments = []\n",
    "\n",
    "        input_csv_filename = f\"translation_{ollama_model_name}_{ollama_prompt_name}.csv\"\n",
    "        \n",
    "        full_input_csv_path = os.path.join(TRANSLATION_FILES_DIR, input_csv_filename)\n",
    "\n",
    "        print(f\"\\n--- Processing file: {full_input_csv_path} ---\")\n",
    "\n",
    "        try:\n",
    "            df_translations_to_judge = pd.read_csv(full_input_csv_path)\n",
    "            print(f\"Successfully loaded {len(df_translations_to_judge)} rows from {full_input_csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: File {full_input_csv_path} not found. Skipping.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {full_input_csv_path}: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if df_translations_to_judge.empty:\n",
    "            print(f\"File {full_input_csv_path} is empty. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # --- START CHAT SESSION FOR EACH FILE ---\n",
    "        if not judge_llm_model: # Check if judge_llm_model was initialized (API key issue)\n",
    "            print(\"ERROR: Judge LLM model not initialized. Cannot proceed with judging. Check API Key.\")\n",
    "            break # Break from the ollama_model_name loop if judge model isn't ready\n",
    "        \n",
    "        try:\n",
    "            chat = judge_llm_model.start_chat(history=[\n",
    "                {'role': 'user', 'parts': [initial_prompt_for_chat]},\n",
    "                {'role': 'model', 'parts': [\"Understood. I am ready to evaluate the sentence pairs based on the provided rubric and will output only a single integer score from 1 to 5 for each. Please provide the first pair.\"]}\n",
    "            ])\n",
    "            print(\"Gemini chat session (re)started with initial instructions and rubric.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not start Gemini chat session for {full_input_csv_path}: {e}. Skipping file.\")\n",
    "            continue # Skip this file if chat can't start\n",
    "        i = 0\n",
    "        # Iterate through rows of the current translation file\n",
    "        for index, row in df_translations_to_judge.iterrows():\n",
    "            archaic_sent = str(row[ARCHAIC_SENTENCE_COLUMN_IN_TRANSLATION_FILES])\n",
    "            modern_trans = str(row[TRANSLATION_COLUMN_IN_TRANSLATION_FILES]) if pd.notna(row[TRANSLATION_COLUMN_IN_TRANSLATION_FILES]) else \"\"\n",
    "            # Data to store for the current row's judgment\n",
    "            judgment_details = {\n",
    "                \"archaic_sentence\": archaic_sent, # Will be renamed to 'Sentence'\n",
    "                \"modern_translation\": modern_trans, # Will be renamed to 'Translation'\n",
    "                \"judge_llm_score_parsed\": None,   # Will be renamed to 'Score'\n",
    "                # Keeping these for debugging, but won't be in the final simple CSV\n",
    "                \"source_csv_debug\": input_csv_filename,\n",
    "                \"ollama_model_debug\": ollama_model_name,\n",
    "                \"ollama_prompt_type_debug\": ollama_prompt_name,\n",
    "                \"judge_llm_raw_output_debug\": \"\",\n",
    "                \"error_message_debug\": None\n",
    "            }\n",
    "\n",
    "            if modern_trans.startswith(\"[ERROR]:\"):\n",
    "                print(f\"  Skipping row {index+1} due to previous translation error: {modern_trans}\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"SKIPPED_OLLAMA_ERROR\"\n",
    "                judgment_details[\"error_message_debug\"] = \"Skipped due to original translation error\"\n",
    "                current_file_judgments.append(judgment_details)\n",
    "                continue\n",
    "\n",
    "            if not modern_trans:\n",
    "                print(f\"  - Row {index + 1}: No modern translation provided. Skipping.\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"NO_TRANSLATION_PROVIDED\"\n",
    "                judgment_details[\"error_message_debug\"] = \"No modern translation in source CSV\"\n",
    "                current_file_judgments.append(judgment_details)\n",
    "                continue\n",
    "\n",
    "            current_pair_prompt = format_individual_judgment_request(archaic_sent, modern_trans)\n",
    "            \n",
    "            try:\n",
    "                response = chat.send_message(\n",
    "                    current_pair_prompt,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings\n",
    "                )\n",
    "                raw_score_text = response.text.strip()\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = raw_score_text\n",
    "\n",
    "                parsed_score = None\n",
    "                match = re.search(r'^\\s*([1-5])\\s*$', raw_score_text)\n",
    "                if match:\n",
    "                    parsed_score = int(match.group(1))\n",
    "                else:\n",
    "                    fallback_match = re.search(r'\\b([1-5])\\b', raw_score_text)\n",
    "                    if fallback_match:\n",
    "                        parsed_score = int(fallback_match.group(1))\n",
    "                        print(f\"    Warning: Parsed score '{parsed_score}' from less strict match. LLM Response: '{raw_score_text}'\")\n",
    "                    else:\n",
    "                        judgment_details[\"error_message_debug\"] = f\"Could not parse a 1-5 score from: '{raw_score_text}'\"\n",
    "                        print(f\"    ERROR: {judgment_details['error_message_debug']}. Raw response: '{raw_score_text}'\")\n",
    "                \n",
    "                judgment_details[\"judge_llm_score_parsed\"] = parsed_score\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR judging translation for row {index + 1}: {e}\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"API_ERROR_DURING_CHAT\"\n",
    "                judgment_details[\"error_message_debug\"] = str(e)\n",
    "            \n",
    "            current_file_judgments.append(judgment_details)\n",
    "\n",
    "            time.sleep(4.3) # Rate limiting for Gemini API\n",
    "        \n",
    "        # After processing all rows for the current input CSV save that\n",
    "        if current_file_judgments:\n",
    "            df_current_judgments = pd.DataFrame(current_file_judgments)\n",
    "            \n",
    "            # Select and rename columns for the final output format\n",
    "            df_output_specific = df_current_judgments[[\n",
    "                \"archaic_sentence\", \n",
    "                \"modern_translation\", \n",
    "                \"judge_llm_score_parsed\"\n",
    "            ]].copy()\n",
    "            \n",
    "            df_output_specific.rename(columns={\n",
    "                \"archaic_sentence\": \"Sentence\",\n",
    "                \"modern_translation\": \"Translation\",\n",
    "                \"judge_llm_score_parsed\": \"Score\"\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Construct the output filename\n",
    "            # translation_model_name is `safe_ollama_model_name`\n",
    "            # prompt is `ollama_prompt_name`\n",
    "            output_filename_specific = f\"scored_{safe_ollama_model_name}_{ollama_prompt_name}.csv\"\n",
    "            full_output_path_specific = os.path.join(JUDGED_OUTPUT_DIR, output_filename_specific)\n",
    "\n",
    "            try:\n",
    "                df_output_specific.to_csv(full_output_path_specific, index=False)\n",
    "                print(f\"Successfully saved judged results to '{full_output_path_specific}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving specific judged results to '{full_output_path_specific}': {e}\")\n",
    "        else:\n",
    "            print(f\"No judgments were made for {full_input_csv_path} (e.g., all rows skipped).\")\n",
    "\n",
    "    if not judge_llm_model: # If API key was bad, no point continuing outer loop\n",
    "        print(\"Halting further processing due to Judge LLM initialization failure.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"\\nOverall Judging process complete. Individual CSVs saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49474474",
   "metadata": {},
   "source": [
    "## Install prometheus 3B from HugginFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "cache_dir = \"./models/m_prometheus_3b\"  #local directory to cache the model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123e925",
   "metadata": {},
   "source": [
    "## Evaluate Translations with prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "judges = [\"gemini\",\"prometheus\"]\n",
    "models = config.get('models', [])\n",
    "prompt_templates = config.get('prompts', [])\n",
    "\n",
    "results = []\n",
    "for judge in judges:\n",
    "    for model_name in models:#iterate through scores\n",
    "        for prompt_name in prompt_templates:\n",
    "            # TODO\n",
    "\n",
    "            if model_name == \"qwen:7B\":\n",
    "                file_path = f\"scores_\"+judge+\"/scored_qwen_{prompt_name}.csv\"\n",
    "                short_model = \"qwen\"\n",
    "            elif model_name == \"galatolo/cerbero-7b\":\n",
    "                file_path = f\"scores_\"+judge+\"/scored_cerbero_{prompt_name}.csv\"\n",
    "                short_model = \"cerbero\"\n",
    "            else:\n",
    "                file_path = f\"scores_{judge}/scored_{model_name}_{prompt_name}.csv\"\n",
    "                short_model = model_name.replace(\"/\", \"\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "            # Check if the file exists and read it\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                if \"score\" in df.columns:\n",
    "                    mean_score = df[\"score\"].mean()# calculate the mean score\n",
    "                    results.append({\n",
    "                        \"model\": short_model,\n",
    "                        \"prompt\": prompt_name,\n",
    "                        \"mean_score\": mean_score\n",
    "                    })\n",
    "                    print(f\"Processing model: {model_name} with prompt: {prompt_name}\")\n",
    "                elif \"Score\" in df.columns:\n",
    "                    mean_score = df[\"Score\"].mean()# calculate the mean score\n",
    "                    results.append({\n",
    "                        \"model\": short_model,\n",
    "                        \"prompt\": prompt_name,\n",
    "                        \"mean_score\": mean_score\n",
    "                    })\n",
    "                    print(f\"Processing model: {model_name} with prompt: {prompt_name}\")\n",
    "                else:\n",
    "                    print(f\"Column'score' not found in {file_path}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"file not found in {file_path}\")\n",
    "\n",
    "    df_results = pd.DataFrame(results)# create a dataframe with the results\n",
    "\n",
    "    os.makedirs(\"scores\", exist_ok=True)\n",
    "    df_results.to_csv(\"final_scores_metrics_\"+ judge +\".csv\", index=False)# save the results to a csv file\n",
    "    results.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import TextGenerationPipeline \n",
    "import tqdm\n",
    "\n",
    "evaluation_prompt_template =\"\"\"Your task is to evaluate the quality of a modern Italian translation based on an original archaic Italian sentence. You must strictly adhere to the provided scoring rubric.\n",
    "\n",
    "Archaic Italian Original:\n",
    "{original}\n",
    "\n",
    "Modern Italian Translation:\n",
    "{translation}\n",
    "\n",
    "Please score the \"Modern Italian Translation\" using the following 1-5 scale, focusing on accuracy, fluency, and preservation of the original meaning:\n",
    "\n",
    "1.  **Completely Unacceptable / Irrelevant:** The translation is nonsensical, completely misses the original meaning, or is entirely irrelevant to the archaic sentence. It may contain severe grammatical errors that render it unintelligible.\n",
    "2.  **Poor - Significant Semantic & Fluency Issues:** The translation has major errors in meaning, significant omissions or additions that distort the original message. It may be difficult to understand due to pervasive awkward phrasing or grammatical mistakes, even if some words are correctly translated.\n",
    "3.  **Fair - Understandable but Flawed:** The translation conveys the general meaning but contains noticeable inaccuracies, awkward or unnatural phrasing, or minor grammatical errors. It lacks the polish and precision of a good translation, and may require mental effort to fully grasp.\n",
    "4.  **Good - Minor Imperfections:** The translation is largely accurate and fluent, successfully conveying the original meaning. Any minor issues are negligible, such as very slight awkwardness in phrasing, minor lexical choices that could be improved, or a minimal loss of subtle nuance.\n",
    "5.  **Excellent - Perfect & Natural:** The translation is highly accurate, perfectly captures the meaning and nuance of the archaic original, and reads as a fluent and natural modern Italian sentence. It demonstrates a complete understanding of both the archaic and modern forms of the language.\n",
    "\n",
    "Based on the above criteria, respond only with the numeric score (1 to 5). Do not include any additional text, explanations, or formatting.\"\"\"\n",
    "\n",
    "judge_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)# Use GPU if available, otherwise CPU\n",
    "\n",
    "# Function to extract score from the model's output\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        score = int(output_text.strip().split()[0])\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "    except:\n",
    "        pass\n",
    "    return \"[INVALID OUTPUT]\"\n",
    "\n",
    "for model_name in models:# iterate through translations\n",
    "    for prompt_name in prompt_templates:\n",
    "        # TODO\n",
    "\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"csvFiles/translations/translation_qwen_{prompt_name}.csv\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"csvFiles/translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            file_path = f\"csvFiles/translations/translation_{model_name}_{prompt_name}.csv\"\n",
    "        df_translation = pd.read_csv(file_path)\n",
    "\n",
    "        scores = []# list to store scores\n",
    "        for _, row in tqdm(df_translation.iterrows(), total=len(df_translation)):\n",
    "            original = row[col_name]\n",
    "            translation = row[\"translation\"]\n",
    "\n",
    "            prompt = evaluation_prompt_template.format(original = original, translation=translation)# create the prompt for evaluation\n",
    "\n",
    "            output = judge_pipeline(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]# generate the output using the model\n",
    "            score = extract_score(output[len(prompt):])  # extract the score from the output\n",
    "            scores.append(score)# append the score to the list\n",
    "\n",
    "        df_translation[\"Score\"] = scores# add the scores to the dataframe\n",
    "        # TODO\n",
    "        #save the dataframe with scores\n",
    "        if model_name == \"qwen:7B\":\n",
    "            df_translation.to_csv(f\"csvFiles/scores_prometheus/scored_qwen_{prompt_name}.csv\", index=False)\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            df_translation.to_csv(f\"csvFiles/scores_prometheus/scored_cerbero_{prompt_name}.csv\", index=False)\n",
    "        else:\n",
    "            df_translation.to_csv(f\"csvFiles/scores_prometheus/scored_{model_name}_{prompt_name}.csv\", index=False)\n",
    "        print(f\"PScores saved in 'scored_{model_name}_{prompt_name}.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
