{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO gemini details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "%pip install -q -U google-generativeai pandas\n",
    "\n",
    "search_for_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1.  **Completely Unacceptable Translation:**\n",
    "    *   The translation has no pertinence with the original meaning.\n",
    "    *   The generated sentence is either gibberish, makes no sense, or is completely unrelated to the archaic text.\n",
    "    *   Contains severe errors that render it incomprehensible or entirely misleading.\n",
    "\n",
    "2.  **Severe Semantic Errors/Omissions:**\n",
    "    *   The translation contains significant semantic errors, critical omissions of meaning from the archaic text, or substantial incorrect additions.\n",
    "    *   While some words might be recognizable, the core meaning is lost or heavily distorted.\n",
    "    *   The modernization is poor, leaving many archaic forms or incorrectly modernizing them.\n",
    "    *   Likely many grammatical errors in modern Italian.\n",
    "\n",
    "3.  **Partially Wrong Translation / Lackluster:**\n",
    "    *   The translation captures some of the original meaning but is lackluster or contains noticeable errors.\n",
    "    *   Errors are mostly minor (e.g., awkward phrasing, typos, minor grammatical mistakes in modern Italian, some less critical semantic misunderstandings or misinterpretations of archaic terms).\n",
    "    *   Some archaic features might be awkwardly modernized or missed.\n",
    "    *   The overall quality is mediocre; it's understandable but clearly flawed.\n",
    "\n",
    "4.  **Good Translation:**\n",
    "    *   The translation is mostly accurate and successfully conveys the core meaning of the archaic sentence.\n",
    "    *   It is substantially faithful to the original text.\n",
    "    *   The modern Italian is fluent and comprehensible.\n",
    "    *   Archaic features are generally well modernized.\n",
    "    *   There might be minor stylistic imperfections (e.g., style doesn't perfectly match natural modern Italian, slight awkwardness) or very minor errors that do not significantly impact understanding or meaning.\n",
    "\n",
    "5.  **Perfect Translation:**\n",
    "    *   The translation is completely accurate, fully conveying the meaning and nuances of the original archaic sentence.\n",
    "    *   It is perfectly fluent, natural-sounding, and grammatically correct modern Italian.\n",
    "    *   All archaic linguistic features (vocabulary, syntax, orthography) are correctly and appropriately modernized.\n",
    "    *   The style is appropriate for modern Italian.\n",
    "    *   No errors.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric Defined.\")\n",
    "\n",
    "\n",
    "def create_judge_prompt(archaic_text, modern_translation, rubric):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "                Your task is to assess the quality of the \"Candidate Modern Translation\" provided below, based on the \"Original Archaic Sentence\".\n",
    "\n",
    "                Please use the following detailed 1-5 scale and rubric for your evaluation:\n",
    "                --- RUBRIC START ---\n",
    "                {rubric}\n",
    "                --- RUBRIC END ---\n",
    "\n",
    "                Original Archaic Sentence:\n",
    "                \"{archaic_text}\"\n",
    "\n",
    "                Candidate Modern Translation:\n",
    "                \"{modern_translation}\"\n",
    "\n",
    "                Carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "                Output ONLY the integer score. Do not add any explanation, prefix, or other text.\n",
    "\n",
    "                Score:\"\"\"\n",
    "\n",
    "print(\"Judge Prompting Function Defined.\")\n",
    "\n",
    "def format_individual_judgment_request(archaic_text, modern_translation):\n",
    "    \"\"\"\n",
    "    Formats the specific archaic-modern pair for an ongoing chat session.\n",
    "    The main rubric and instructions are assumed to be set in the chat history.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "Now, please evaluate the following pair:\n",
    "\n",
    "Original Archaic Sentence:\n",
    "\"{archaic_text}\"\n",
    "\n",
    "Candidate Modern Translation:\n",
    "\"{modern_translation}\"\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "print(\"Individual judgment request formatter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2575779",
   "metadata": {},
   "source": [
    "Config the API and the models and prompts names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List gemini models available\n",
    "if search_for_models:  # Set to True to list available models\n",
    "  import google.generativeai as genai\n",
    "\n",
    "  print(\"Available Gemini models that supports 'generateContent':\")\n",
    "  for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "      print(f\"- Name: {m.name}, Display Name: {m.display_name}, Description: {m.description}\")\n",
    "      # print(f\"  Supported methods: {m.supported_generation_methods}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ab30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell set up Gemini and prepare to loop through translation CSVs\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import configparser \n",
    "import json\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONFIG_API_FILE_PATH = \"gemini_config.ini\" # Path to configuration file\n",
    "API_KEY = None\n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "# Read API Key from gemini config file\n",
    "config = configparser.ConfigParser()\n",
    "if os.path.exists(CONFIG_API_FILE_PATH):\n",
    "    try:\n",
    "        config.read(CONFIG_API_FILE_PATH)\n",
    "        API_KEY = config.get('GEMINI', 'API_KEY', fallback=None)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read API key from {CONFIG_API_FILE_PATH}. Error: {e}\")\n",
    "else:\n",
    "    print(f\"ERROR: Configuration file {CONFIG_API_FILE_PATH} not found.\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"ERROR: Gemini API Key not found or not set correctly in gemini_config.ini.\")\n",
    "    # Raise an exception here\n",
    "    raise ValueError(\"Gemini API Key not configured. Halting execution.\")\n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        print(\"Gemini API Key configured successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to configure Gemini API with the provided key. Error: {e}\")\n",
    "        raise ValueError(\"Gemini API Key configuration failed. Halting execution.\")\n",
    "\n",
    "JUDGE_MODEL_NAME = \"models/gemini-1.5-flash-latest\" # Suitable Gemini model\n",
    "TRANSLATION_FILES_DIR = \"translations/\" # Directory where translation_model.csv are saved \n",
    "ARCHAIC_SENTENCE_COLUMN_IN_TRANSLATION_FILES = \"Sentence\" # Column name of archaic sentences in translation CSVs\n",
    "TRANSLATION_COLUMN_IN_TRANSLATION_FILES = \"translation\"  # Column name of the LLM's translation in translation CSVs\n",
    "\n",
    "\n",
    "\n",
    "# Models and prompt types you used with Ollama (to find the CSV files)\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    ollama_models_used = config.get('models', [])\n",
    "    ollama_prompt_names_used = config.get('prompts', [])\n",
    "\n",
    "    print(\"Config success:\")\n",
    "    print(f\"Models: {ollama_models_used}\")\n",
    "    print(f\"Prompts: {ollama_prompt_names_used}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Config file '{CONFIG_FILE_PATH}' not found.\")\n",
    "    # Puoi anche uscire o impostare valori di default qui\n",
    "    ollama_models_used = []\n",
    "    ollama_prompt_names_used = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: File '{CONFIG_FILE_PATH}' not a valid JSON.\")\n",
    "    ollama_models_used = []\n",
    "    ollama_prompt_names_used = []\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Models used: {ollama_models_used}\")\n",
    "\n",
    "\n",
    "all_judgments_data = [] # This will store judgments from ALL files\n",
    "\n",
    "print(\"Gemini Setup Complete.\")\n",
    "print(f\"Judge Model: {JUDGE_MODEL_NAME}\")\n",
    "print(f\"Will look for translation CSVs in: {TRANSLATION_FILES_DIR}\")\n",
    "\n",
    "# --- SAFETY AND GENERATION CONFIG (can be in the same cell or next) ---\n",
    "# (This part can be taken from your cell e8a09461, it's good)\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    # temperature=0.1 # For more deterministic output from the judge\n",
    ")\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "# Initialize the generative model for the judge\n",
    "judge_llm_model = genai.GenerativeModel(JUDGE_MODEL_NAME) # Renamed to avoid conflict with ollama 'model' variable\n",
    "print(f\"Using Judge LLM: {judge_llm_model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70992d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will contain the main judging loop\n",
    "import sys\n",
    "# --- START A CHAT SESSION WITH GEMINI --- (This part is conceptual for the loop, actual start is inside)\n",
    "initial_prompt_for_chat = f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "Your consistent task throughout this session is to assess the quality of \"Candidate Modern Translation\"s provided to you, based on their corresponding \"Original Archaic Sentence\"s.\n",
    "\n",
    "Please use the following detailed 1-5 scale and rubric for ALL your evaluations in this session:\n",
    "--- RUBRIC START ---\n",
    "{JUDGE_RUBRIC}\n",
    "--- RUBRIC END ---\n",
    "\n",
    "For each pair I provide, carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "Output ONLY the integer score for each. Do not add any explanation, prefix, or other text.\n",
    "I will provide you with sentence pairs one by one. Let's begin.\n",
    "\"\"\"\n",
    "\n",
    "# Define the directory for saving judged outputs\n",
    "JUDGED_OUTPUT_DIR = \"scores_gemini/\" # You can change this path\n",
    "os.makedirs(JUDGED_OUTPUT_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# judge_model_name for the filename (e.g., \"gemini-pro\")\n",
    "# We can get this from JUDGE_MODEL_NAME or judge_llm_model.model_name\n",
    "# Let's sanitize it to be filename-friendly (replace slashes etc.)\n",
    "safe_judge_model_name = JUDGE_MODEL_NAME.replace(\"/\", \"-\").replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "# Loop through each combination of Ollama model and prompt type to load the corresponding CSV\n",
    "for ollama_model_name in ollama_models_used:\n",
    "    # Sanitize ollama_model_name for filename (e.g., if it's \"qwen:7B\", make it \"qwen-7B\")\n",
    "    safe_ollama_model_name = ollama_model_name.replace(\":\", \"-\").replace(\"/\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "\n",
    "    for ollama_prompt_name in ollama_prompt_names_used:\n",
    "        # This list will store judgments for the CURRENT file/combination\n",
    "        current_file_judgments = []\n",
    "\n",
    "        input_csv_filename = f\"translation_{ollama_model_name}_{ollama_prompt_name}.csv\"\n",
    "        \n",
    "        full_input_csv_path = os.path.join(TRANSLATION_FILES_DIR, input_csv_filename)\n",
    "\n",
    "        print(f\"\\n--- Processing file: {full_input_csv_path} ---\")\n",
    "\n",
    "        try:\n",
    "            df_translations_to_judge = pd.read_csv(full_input_csv_path)\n",
    "            print(f\"Successfully loaded {len(df_translations_to_judge)} rows from {full_input_csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: File {full_input_csv_path} not found. Skipping.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {full_input_csv_path}: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if df_translations_to_judge.empty:\n",
    "            print(f\"File {full_input_csv_path} is empty. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # --- (RE)START CHAT SESSION FOR EACH FILE ---\n",
    "        if not judge_llm_model: # Check if judge_llm_model was initialized (API key issue)\n",
    "            print(\"ERROR: Judge LLM model not initialized. Cannot proceed with judging. Check API Key.\")\n",
    "            break # Break from the ollama_model_name loop if judge model isn't ready\n",
    "        \n",
    "        try:\n",
    "            chat = judge_llm_model.start_chat(history=[\n",
    "                {'role': 'user', 'parts': [initial_prompt_for_chat]},\n",
    "                {'role': 'model', 'parts': [\"Understood. I am ready to evaluate the sentence pairs based on the provided rubric and will output only a single integer score from 1 to 5 for each. Please provide the first pair.\"]}\n",
    "            ])\n",
    "            print(\"Gemini chat session (re)started with initial instructions and rubric.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not start Gemini chat session for {full_input_csv_path}: {e}. Skipping file.\")\n",
    "            continue # Skip this file if chat can't start\n",
    "        i = 0\n",
    "        # Iterate through rows of the current translation file\n",
    "        for index, row in df_translations_to_judge.iterrows():\n",
    "            archaic_sent = str(row[ARCHAIC_SENTENCE_COLUMN_IN_TRANSLATION_FILES])\n",
    "            modern_trans = str(row[TRANSLATION_COLUMN_IN_TRANSLATION_FILES]) if pd.notna(row[TRANSLATION_COLUMN_IN_TRANSLATION_FILES]) else \"\"\n",
    "            # Data to store for the current row's judgment (will be part of current_file_judgments)\n",
    "            judgment_details = {\n",
    "                \"archaic_sentence\": archaic_sent, # Will be renamed to 'Sentence'\n",
    "                \"modern_translation\": modern_trans, # Will be renamed to 'Translation'\n",
    "                \"judge_llm_score_parsed\": None,   # Will be renamed to 'Vote'\n",
    "                # --- Keeping these for debugging, but won't be in the final simple CSV ---\n",
    "                \"source_csv_debug\": input_csv_filename,\n",
    "                \"ollama_model_debug\": ollama_model_name,\n",
    "                \"ollama_prompt_type_debug\": ollama_prompt_name,\n",
    "                \"judge_llm_raw_output_debug\": \"\",\n",
    "                \"error_message_debug\": None\n",
    "            }\n",
    "\n",
    "            if modern_trans.startswith(\"[ERROR]:\"):\n",
    "                print(f\"  Skipping row {index+1} due to previous translation error: {modern_trans}\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"SKIPPED_OLLAMA_ERROR\"\n",
    "                judgment_details[\"error_message_debug\"] = \"Skipped due to original translation error\"\n",
    "                current_file_judgments.append(judgment_details)\n",
    "                continue\n",
    "\n",
    "            if not modern_trans:\n",
    "                print(f\"  - Row {index + 1}: No modern translation provided. Skipping.\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"NO_TRANSLATION_PROVIDED\"\n",
    "                judgment_details[\"error_message_debug\"] = \"No modern translation in source CSV\"\n",
    "                current_file_judgments.append(judgment_details)\n",
    "                continue\n",
    "\n",
    "            current_pair_prompt = format_individual_judgment_request(archaic_sent, modern_trans)\n",
    "            \n",
    "            try:\n",
    "                response = chat.send_message(\n",
    "                    current_pair_prompt,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings\n",
    "                )\n",
    "                raw_score_text = response.text.strip()\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = raw_score_text\n",
    "\n",
    "                parsed_score = None\n",
    "                match = re.search(r'^\\s*([1-5])\\s*$', raw_score_text)\n",
    "                if match:\n",
    "                    parsed_score = int(match.group(1))\n",
    "                else:\n",
    "                    fallback_match = re.search(r'\\b([1-5])\\b', raw_score_text)\n",
    "                    if fallback_match:\n",
    "                        parsed_score = int(fallback_match.group(1))\n",
    "                        print(f\"    Warning: Parsed score '{parsed_score}' from less strict match. LLM Response: '{raw_score_text}'\")\n",
    "                    else:\n",
    "                        judgment_details[\"error_message_debug\"] = f\"Could not parse a 1-5 score from: '{raw_score_text}'\"\n",
    "                        print(f\"    ERROR: {judgment_details['error_message_debug']}. Raw response: '{raw_score_text}'\")\n",
    "                \n",
    "                judgment_details[\"judge_llm_score_parsed\"] = parsed_score\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR judging translation for row {index + 1}: {e}\")\n",
    "                judgment_details[\"judge_llm_raw_output_debug\"] = \"API_ERROR_DURING_CHAT\"\n",
    "                judgment_details[\"error_message_debug\"] = str(e)\n",
    "            \n",
    "            current_file_judgments.append(judgment_details)\n",
    "            max_msg_len = len(f\"Elaborato: 100/100 elementi...\")\n",
    "            i= i + 1\n",
    "    \n",
    "            # Costruisci il messaggio\n",
    "            message = f\"Elaborato: {i}/100 elementi...\"\n",
    "        \n",
    "            # Pulisci la riga precedente riempiendo con spazi fino alla lunghezza massima prevista\n",
    "            # e poi sposta il cursore all'inizio della riga con '\\r'\n",
    "            sys.stdout.write(f\"\\r{message.ljust(max_msg_len)}\")\n",
    "        \n",
    "            # Forza la scrittura su stdout (necessario per aggiornare in tempo reale)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            time.sleep(4.3) # Rate limiting if needed\n",
    "        \n",
    "        # --- After processing all rows for the current input CSV ---\n",
    "        if current_file_judgments:\n",
    "            df_current_judgments = pd.DataFrame(current_file_judgments)\n",
    "            \n",
    "            # Select and rename columns for the final output format\n",
    "            df_output_specific = df_current_judgments[[\n",
    "                \"archaic_sentence\", \n",
    "                \"modern_translation\", \n",
    "                \"judge_llm_score_parsed\"\n",
    "            ]].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "            \n",
    "            df_output_specific.rename(columns={\n",
    "                \"archaic_sentence\": \"Sentence\",\n",
    "                \"modern_translation\": \"Translation\",\n",
    "                \"judge_llm_score_parsed\": \"Score\"\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Construct the output filename\n",
    "            # judge_model_name is `safe_judge_model_name`\n",
    "            # translation_model_name is `safe_ollama_model_name`\n",
    "            # prompt is `ollama_prompt_name`\n",
    "            output_filename_specific = f\"judge_{safe_judge_model_name}_{safe_ollama_model_name}_{ollama_prompt_name}.csv\"\n",
    "            full_output_path_specific = os.path.join(JUDGED_OUTPUT_DIR, output_filename_specific)\n",
    "\n",
    "            try:\n",
    "                df_output_specific.to_csv(full_output_path_specific, index=False)\n",
    "                print(f\"Successfully saved judged results to '{full_output_path_specific}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving specific judged results to '{full_output_path_specific}': {e}\")\n",
    "        else:\n",
    "            print(f\"No judgments were made for {full_input_csv_path} (e.g., all rows skipped).\")\n",
    "\n",
    "    if not judge_llm_model: # If API key was bad, no point continuing outer loop\n",
    "        print(\"Halting further processing due to Judge LLM initialization failure.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"\\nOverall Judging process complete. Individual CSVs saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
