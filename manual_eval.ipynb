{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd25f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateCSVwithSamples = False\n",
    "merge_csv_files = False\n",
    "llm_vs_manual = True\n",
    "llm_to_compare = \"prometheus\" # \"prometheus\" or \"gemini\"\n",
    "\n",
    "# Read config json file\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "config_file = \"config.json\"\n",
    "if not os.path.exists(config_file):\n",
    "    print(f\"Config file not found: {config_file}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "models = config.get('models', [])\n",
    "prompts = config.get('prompts', [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9205cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_federico_path = \"manual_evaluate_translations_federico/\"\n",
    "folder_gianmarco_path = \"manual_evaluate_translations_gianmarco/\"\n",
    "\n",
    "if generateCSVwithSamples:\n",
    "    input_folder_path = \"translations/\"\n",
    "\n",
    "    os.makedirs(folder_federico_path, exist_ok=True)\n",
    "    os.makedirs(folder_gianmarco_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            input_path = os.path.join(input_folder_path, filename)\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Extract first 10 rows\n",
    "            first_10 = df.head(10)\n",
    "            output_federico = os.path.join(folder_federico_path, filename)\n",
    "            first_10.to_csv(output_federico, index=False)\n",
    "            \n",
    "            # Extract last 10 rows\n",
    "            last_10 = df.tail(10)\n",
    "            output_gianmarco = os.path.join(folder_gianmarco_path, filename)\n",
    "            last_10.to_csv(output_gianmarco, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82edd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged scored_cerbero_base.csv from both folders and saved to scores_manual/scored_cerbero_base.csv\n",
      "Merged scored_cerbero_detailed.csv from both folders and saved to scores_manual/scored_cerbero_detailed.csv\n",
      "Merged scored_cerbero_few_shot.csv from both folders and saved to scores_manual/scored_cerbero_few_shot.csv\n",
      "Merged scored_cerbero_role-based.csv from both folders and saved to scores_manual/scored_cerbero_role-based.csv\n",
      "Merged scored_cerbero_teacher_student.csv from both folders and saved to scores_manual/scored_cerbero_teacher_student.csv\n",
      "Merged scored_gemma_base.csv from both folders and saved to scores_manual/scored_gemma_base.csv\n",
      "Merged scored_gemma_detailed.csv from both folders and saved to scores_manual/scored_gemma_detailed.csv\n",
      "Merged scored_gemma_few_shot.csv from both folders and saved to scores_manual/scored_gemma_few_shot.csv\n",
      "Merged scored_gemma_role-based.csv from both folders and saved to scores_manual/scored_gemma_role-based.csv\n",
      "Merged scored_gemma_teacher_student.csv from both folders and saved to scores_manual/scored_gemma_teacher_student.csv\n",
      "Merged scored_llama3_base.csv from both folders and saved to scores_manual/scored_llama3_base.csv\n",
      "Merged scored_llama3_detailed.csv from both folders and saved to scores_manual/scored_llama3_detailed.csv\n",
      "Merged scored_llama3_few_shot.csv from both folders and saved to scores_manual/scored_llama3_few_shot.csv\n",
      "Merged scored_llama3_role-based.csv from both folders and saved to scores_manual/scored_llama3_role-based.csv\n",
      "Merged scored_llama3_teacher_student.csv from both folders and saved to scores_manual/scored_llama3_teacher_student.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge the csv files from both folders into a single CSV file\n",
    "def merge_csv_files(folder_path1, folder_path2):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for filename in os.listdir(folder_path1):\n",
    "        if filename.endswith('.csv'):\n",
    "            if filename in os.listdir(folder_path2):\n",
    "                path1 = os.path.join(folder_path1, filename)\n",
    "                path2 = os.path.join(folder_path2, filename)\n",
    "                \n",
    "                df1 = pd.read_csv(path1)\n",
    "                df2 = pd.read_csv(path2)\n",
    "                \n",
    "                # Merge the two dataframes\n",
    "                merged_df = pd.concat([merged_df, df1, df2], ignore_index=True)\n",
    "                # Save as csv\n",
    "                #new file name\n",
    "                filename = filename.replace('translation', 'scored')\n",
    "                output_path = os.path.join(\"scores_manual/\", filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                merged_df.to_csv(output_path, index=False)\n",
    "                print(f\"Merged {filename} from both folders and saved to {output_path}\")\n",
    "                # Clear the merged_df for the next file\n",
    "                merged_df = pd.DataFrame()\n",
    "\n",
    "if merge_csv_files:\n",
    "    # Merge the CSV files from both folders\n",
    "    merged_df = merge_csv_files(folder_federico_path, folder_gianmarco_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75d1b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counters initialized: {'cerbero': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'gemma': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'llama3': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}}\n",
      "\n",
      "Counters in a readable format:\n",
      "\n",
      "Model: cerbero\n",
      "  Prompt: base, Count: 6\n",
      "  Prompt: detailed, Count: 4\n",
      "  Prompt: few_shot, Count: 4\n",
      "  Prompt: role-based, Count: 2\n",
      "  Prompt: teacher_student, Count: 8\n",
      "\n",
      "Model: gemma\n",
      "  Prompt: base, Count: 5\n",
      "  Prompt: detailed, Count: 3\n",
      "  Prompt: few_shot, Count: 6\n",
      "  Prompt: role-based, Count: 4\n",
      "  Prompt: teacher_student, Count: 7\n",
      "\n",
      "Model: llama3\n",
      "  Prompt: base, Count: 5\n",
      "  Prompt: detailed, Count: 7\n",
      "  Prompt: few_shot, Count: 8\n",
      "  Prompt: role-based, Count: 8\n",
      "  Prompt: teacher_student, Count: 8\n",
      "\n",
      "Variance scores:\n",
      "\n",
      "Model: cerbero\n",
      "  Prompt: base, Variance: 1.1500\n",
      "  Prompt: detailed, Variance: 1.6500\n",
      "  Prompt: few_shot, Variance: 1.3000\n",
      "  Prompt: role-based, Variance: 1.3000\n",
      "  Prompt: teacher_student, Variance: 1.3750\n",
      "\n",
      "Model: gemma\n",
      "  Prompt: base, Variance: 1.6750\n",
      "  Prompt: detailed, Variance: 1.9000\n",
      "  Prompt: few_shot, Variance: 0.9250\n",
      "  Prompt: role-based, Variance: 1.1750\n",
      "  Prompt: teacher_student, Variance: 0.8500\n",
      "\n",
      "Model: llama3\n",
      "  Prompt: base, Variance: 0.7000\n",
      "  Prompt: detailed, Variance: 0.7750\n",
      "  Prompt: few_shot, Variance: 0.6500\n",
      "  Prompt: role-based, Variance: 0.9500\n",
      "  Prompt: teacher_student, Variance: 0.9250\n",
      "Counters initialized: {'cerbero': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'gemma': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'llama3': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}}\n",
      "\n",
      "Counters in a readable format:\n",
      "\n",
      "Model: cerbero\n",
      "  Prompt: base, Count: 7\n",
      "  Prompt: detailed, Count: 6\n",
      "  Prompt: few_shot, Count: 7\n",
      "  Prompt: role-based, Count: 0\n",
      "  Prompt: teacher_student, Count: 2\n",
      "\n",
      "Model: gemma\n",
      "  Prompt: base, Count: 6\n",
      "  Prompt: detailed, Count: 6\n",
      "  Prompt: few_shot, Count: 10\n",
      "  Prompt: role-based, Count: 0\n",
      "  Prompt: teacher_student, Count: 6\n",
      "\n",
      "Model: llama3\n",
      "  Prompt: base, Count: 2\n",
      "  Prompt: detailed, Count: 5\n",
      "  Prompt: few_shot, Count: 8\n",
      "  Prompt: role-based, Count: 9\n",
      "  Prompt: teacher_student, Count: 2\n",
      "\n",
      "Variance scores:\n",
      "\n",
      "Model: cerbero\n",
      "  Prompt: base, Variance: 0.9750\n",
      "  Prompt: detailed, Variance: 1.4500\n",
      "  Prompt: few_shot, Variance: 0.7750\n",
      "  Prompt: role-based, Variance: No data\n",
      "  Prompt: teacher_student, Variance: 1.3000\n",
      "\n",
      "Model: gemma\n",
      "  Prompt: base, Variance: 0.8500\n",
      "  Prompt: detailed, Variance: 0.5000\n",
      "  Prompt: few_shot, Variance: 0.4000\n",
      "  Prompt: role-based, Variance: No data\n",
      "  Prompt: teacher_student, Variance: 0.3500\n",
      "\n",
      "Model: llama3\n",
      "  Prompt: base, Variance: 1.2500\n",
      "  Prompt: detailed, Variance: 0.8000\n",
      "  Prompt: few_shot, Variance: 0.6500\n",
      "  Prompt: role-based, Variance: 0.3500\n",
      "  Prompt: teacher_student, Variance: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "manual_eval_path = \"scores_manual/\"\n",
    "llm_eval_path =\"scores_prometheus/\"\n",
    "\n",
    "def compare_scores(manual_eval_path, llm_eval_path):\n",
    "    # Check if the manual evaluation path exists\n",
    "    if not os.path.exists(manual_eval_path):\n",
    "        print(f\"Manual evaluation path does not exist: {manual_eval_path}\")\n",
    "        return\n",
    "    \n",
    "    # Check if the LLM evaluation path exists\n",
    "    if not os.path.exists(llm_eval_path):\n",
    "        print(f\"LLM evaluation path does not exist: {llm_eval_path}\")\n",
    "        return\n",
    "\n",
    "    # dictiornary for each model and prompt in config there is a value counter, initialize that as 0\n",
    "    counters = {model: {prompt: 0 for prompt in prompts} for model in models}\n",
    "    # Variance for each model and prompt\n",
    "    scores_variance = {model: {prompt: [] for prompt in prompts} for model in models}\n",
    "    print(\"Counters initialized:\", counters)\n",
    "\n",
    "    if llm_vs_manual:\n",
    "        # Load the manual evaluation scores\n",
    "        manual_files = [f for f in os.listdir(manual_eval_path) if f.endswith('.csv')]\n",
    "        llm_files = [f for f in os.listdir(llm_eval_path) if f.endswith('.csv')]\n",
    "\n",
    "        for manual_file in manual_files:\n",
    "            if manual_file in llm_files:\n",
    "                manual_df = pd.read_csv(os.path.join(manual_eval_path, manual_file))\n",
    "                llm_df = pd.read_csv(os.path.join(llm_eval_path, manual_file))\n",
    "                # Extract model and prompt from the filename (file format is scored_model_prompt.csv)\n",
    "                file = manual_file.replace('scored_', '').replace('.csv', '')\n",
    "                model, prompt = file.split('_', 1)\n",
    "                # Compare the item in manual_df with llm_df\n",
    "                curr_variance = []\n",
    "                for index, row in manual_df.iterrows():\n",
    "                    \n",
    "                    score_manual = row['Score']\n",
    "                    \n",
    "                    # Find the corresponding row in llm_df using the Sentence coulmn\n",
    "                    llm_row = llm_df[llm_df['Sentence'] == row['Sentence']]\n",
    "                    \n",
    "                    if not llm_row.empty:\n",
    "                        score_llm = llm_row['Score'].values[0]\n",
    "                        # Compare scores\n",
    "                        if score_manual == score_llm:\n",
    "                            counters[model][prompt] += 1\n",
    "                            # print(f\"Manual score {score_manual} is equal to LLM score {score_llm} for model {model} and prompt {prompt}\")\n",
    "                        variance = (score_manual - score_llm) ** 2/ 2  # Variance calculation\n",
    "                        curr_variance.append(variance) \n",
    "                        #print variance\n",
    "                        # print(f\"Manual score {score_manual}, LLM score {score_llm}, Variance: {variance:.4f} for model {model} and prompt {prompt}\")\n",
    "                scores_variance[model][prompt].append(np.mean(curr_variance))\n",
    "    \n",
    "    return counters, scores_variance\n",
    "\n",
    "# # Print the final counters\n",
    "# print(\"Final counters:\")\n",
    "# for model, prompts in counters.items():\n",
    "#     for prompt, count in prompts.items():\n",
    "#         print(f\"Model: {model}, Prompt: {prompt}, Count: {count}\")\n",
    "\n",
    "\n",
    "counters, scores_variance = compare_scores(manual_eval_path, llm_eval_path)\n",
    "\n",
    "\n",
    "#print counters readable format\n",
    "print(\"\\nCounters in a readable format:\")\n",
    "for model, prompts in counters.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, count in prompts.items():\n",
    "        print(f\"  Prompt: {prompt}, Count: {count}\")\n",
    "\n",
    "# Print the variance scores\n",
    "print(\"\\nVariance scores:\")\n",
    "for model, prompts in scores_variance.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, variances in prompts.items():\n",
    "        if variances:\n",
    "            print(f\"  Prompt: {prompt}, Variance: {np.mean(variances):.4f}\")\n",
    "        else:\n",
    "            print(f\"  Prompt: {prompt}, Variance: No data\")\n",
    "\n",
    "\n",
    "# Insert all the data (variance and counters) in a csv Report.csv \n",
    "report_data = []\n",
    "\n",
    "\n",
    "for model, prompts in counters.items():\n",
    "    for prompt, count in prompts.items():\n",
    "        #Float the variance for each model and prompt\n",
    "        variance = np.mean(scores_variance[model][prompt])\n",
    "        report_data.append({\n",
    "            'Model': model,\n",
    "            'Prompt': prompt,\n",
    "            'Judge' : llm_eval_path.split('_')[1].replace('/', ''),  # Extract judge from path\n",
    "            'Same_Scores': count,\n",
    "            'Variance': variance\n",
    "        })\n",
    "\n",
    "llm_eval_path = \"scores_gemini/\"\n",
    "\n",
    "counters, scores_variance = compare_scores(manual_eval_path, llm_eval_path)\n",
    "\n",
    "for model, prompts in counters.items():\n",
    "    for prompt, count in prompts.items():\n",
    "        #Float the variance for each model and prompt\n",
    "        variance = np.mean(scores_variance[model][prompt])\n",
    "        report_data.append({\n",
    "            'Model': model,\n",
    "            'Prompt': prompt,\n",
    "            'Judge' : llm_eval_path.split('_')[1].replace('/', ''),  # Extract judge from path\n",
    "            'Same_Scores': count,\n",
    "            'Variance': variance\n",
    "        })\n",
    "\n",
    "        \n",
    "\n",
    "#print counters readable format\n",
    "print(\"\\nCounters in a readable format:\")\n",
    "for model, prompts in counters.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, count in prompts.items():\n",
    "        print(f\"  Prompt: {prompt}, Count: {count}\")\n",
    "\n",
    "# Print the variance scores\n",
    "print(\"\\nVariance scores:\")\n",
    "for model, prompts in scores_variance.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, variances in prompts.items():\n",
    "        if variances:\n",
    "            print(f\"  Prompt: {prompt}, Variance: {np.mean(variances):.4f}\")\n",
    "        else:\n",
    "            print(f\"  Prompt: {prompt}, Variance: No data\")\n",
    "\n",
    "\n",
    "\n",
    "report_df = pd.DataFrame(report_data)\n",
    "report_df.to_csv('report.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
