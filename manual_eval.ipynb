{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ab04b3",
   "metadata": {},
   "source": [
    "# Prepare Manual Evaluation and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c25b01",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd25f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateCSVwithSamples = False\n",
    "merge_csv_files = False\n",
    "llm_vs_manual = True\n",
    "llm_to_compare = \"gemini\" # \"prometheus\" or \"gemini\"\n",
    "\n",
    "# Read config json file\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "config_file = \"config.json\"\n",
    "if not os.path.exists(config_file):\n",
    "    print(f\"Config file not found: {config_file}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "models = config.get('models', [])\n",
    "prompts = config.get('prompts', [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2e25a",
   "metadata": {},
   "source": [
    "## Prepare Manual eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9205cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_federico_path = \"csvFiles/manual_evaluate_translations_federico/\"\n",
    "folder_gianmarco_path = \"csvFiles/manual_evaluate_translations_gianmarco/\"\n",
    "\n",
    "if generateCSVwithSamples:\n",
    "    input_folder_path = \"csvFiles/translations/\"\n",
    "\n",
    "    os.makedirs(folder_federico_path, exist_ok=True)\n",
    "    os.makedirs(folder_gianmarco_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            input_path = os.path.join(input_folder_path, filename)\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Extract first 10 rows\n",
    "            first_10 = df.head(10)\n",
    "            output_federico = os.path.join(folder_federico_path, filename)\n",
    "            first_10.to_csv(output_federico, index=False)\n",
    "            \n",
    "            # Extract last 10 rows\n",
    "            last_10 = df.tail(10)\n",
    "            output_gianmarco = os.path.join(folder_gianmarco_path, filename)\n",
    "            last_10.to_csv(output_gianmarco, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the csv files from both folders into a single CSV file\n",
    "def merge_csv_files(folder_path1, folder_path2):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for filename in os.listdir(folder_path1):\n",
    "        if filename.endswith('.csv'):\n",
    "            if filename in os.listdir(folder_path2):\n",
    "                path1 = os.path.join(folder_path1, filename)\n",
    "                path2 = os.path.join(folder_path2, filename)\n",
    "                \n",
    "                df1 = pd.read_csv(path1)\n",
    "                df2 = pd.read_csv(path2)\n",
    "                \n",
    "                # Merge the two dataframes\n",
    "                merged_df = pd.concat([merged_df, df1, df2], ignore_index=True)\n",
    "                # Save as csv\n",
    "                #new file name\n",
    "                filename = filename.replace('translation', 'scored')\n",
    "                output_path = os.path.join(\"csvFiles/scores_manual/\", filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                merged_df.to_csv(output_path, index=False)\n",
    "                print(f\"Merged {filename} from both folders and saved to {output_path}\")\n",
    "                # Clear the merged_df for the next file\n",
    "                merged_df = pd.DataFrame()\n",
    "\n",
    "if merge_csv_files:\n",
    "    # Merge the CSV files from both folders\n",
    "    merged_df = merge_csv_files(folder_federico_path, folder_gianmarco_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33f376",
   "metadata": {},
   "source": [
    "## Compare Manual vs LLM-Judge scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "manual_eval_path = \"csvFiles/scores_manual/\"\n",
    "llm_eval_path =\"csvFiles/scores_prometheus/\"\n",
    "\n",
    "def compare_scores(manual_eval_path, llm_eval_path):\n",
    "    # Check if the manual evaluation path exists\n",
    "    if not os.path.exists(manual_eval_path):\n",
    "        print(f\"Manual evaluation path does not exist: {manual_eval_path}\")\n",
    "        return\n",
    "    \n",
    "    # Check if the LLM evaluation path exists\n",
    "    if not os.path.exists(llm_eval_path):\n",
    "        print(f\"LLM evaluation path does not exist: {llm_eval_path}\")\n",
    "        return\n",
    "\n",
    "    # dictiornary for each model and prompt in config there is a value counter, initialize that as 0\n",
    "    counters = {model: {prompt: 0 for prompt in prompts} for model in models}\n",
    "    # Variance for each model and prompt\n",
    "    scores_variance = {model: {prompt: [] for prompt in prompts} for model in models}\n",
    "    print(\"Counters initialized:\", counters)\n",
    "\n",
    "    if llm_vs_manual:\n",
    "        # Load the manual evaluation scores\n",
    "        manual_files = [f for f in os.listdir(manual_eval_path) if f.endswith('.csv')]\n",
    "        llm_files = [f for f in os.listdir(llm_eval_path) if f.endswith('.csv')]\n",
    "\n",
    "        for manual_file in manual_files:\n",
    "            if manual_file in llm_files:\n",
    "                manual_df = pd.read_csv(os.path.join(manual_eval_path, manual_file))\n",
    "                llm_df = pd.read_csv(os.path.join(llm_eval_path, manual_file))\n",
    "                # Extract model and prompt from the filename (file format is scored_model_prompt.csv)\n",
    "                file = manual_file.replace('scored_', '').replace('.csv', '')\n",
    "                model, prompt = file.split('_', 1)\n",
    "                # Compare the item in manual_df with llm_df\n",
    "                curr_diffs = []\n",
    "                for index, row in manual_df.iterrows():\n",
    "                    \n",
    "                    score_manual = row['Score']\n",
    "                    \n",
    "                    # Find the corresponding row in llm_df using the Sentence coulmn\n",
    "                    llm_row = llm_df[llm_df['Sentence'] == row['Sentence']]\n",
    "                    \n",
    "                    if not llm_row.empty:\n",
    "                        score_llm = llm_row['Score'].values[0]\n",
    "                        # Compare scores\n",
    "                        if score_manual == score_llm:\n",
    "                            counters[model][prompt] += 1\n",
    "                        curr_diffs.append(score_manual - score_llm)\n",
    "\n",
    "                if curr_diffs:\n",
    "                    # ddof=1 is the sample variance\n",
    "                    variance = np.var(curr_diffs, ddof=1)\n",
    "                    scores_variance[model][prompt].append(variance)\n",
    "    \n",
    "    return counters, scores_variance\n",
    "\n",
    "counters, scores_variance = compare_scores(manual_eval_path, llm_eval_path)\n",
    "\n",
    "\n",
    "#print counters readable format\n",
    "print(\"\\nCounters in a readable format:\")\n",
    "for model, prompts in counters.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, count in prompts.items():\n",
    "        print(f\"  Prompt: {prompt}, Count: {count}\")\n",
    "\n",
    "# Print the variance scores\n",
    "print(\"\\nVariance scores:\")\n",
    "for model, prompts in scores_variance.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, variances in prompts.items():\n",
    "        if variances:\n",
    "            print(f\"  Prompt: {prompt}, Variance: {np.mean(variances):.4f}\")\n",
    "        else:\n",
    "            print(f\"  Prompt: {prompt}, Variance: No data\")\n",
    "\n",
    "\n",
    "# Insert all the data (variance and counters) in a csv Report.csv \n",
    "report_data = []\n",
    "\n",
    "\n",
    "for model, prompts in counters.items():\n",
    "    for prompt, count in prompts.items():\n",
    "        #Float the variance for each model and prompt\n",
    "        variance = np.mean(scores_variance[model][prompt])\n",
    "        report_data.append({\n",
    "            'Model': model,\n",
    "            'Prompt': prompt,\n",
    "            'Judge' : llm_eval_path.split('_')[1].replace('/', ''),  # Extract judge from path\n",
    "            'Same_Scores': count,\n",
    "            'Variance': variance\n",
    "        })\n",
    "\n",
    "llm_eval_path = \"csvFiles/scores_gemini/\"\n",
    "\n",
    "counters, scores_variance = compare_scores(manual_eval_path, llm_eval_path)\n",
    "\n",
    "for model, prompts in counters.items():\n",
    "    for prompt, count in prompts.items():\n",
    "        #Float the variance for each model and prompt\n",
    "        variance = np.mean(scores_variance[model][prompt])\n",
    "        report_data.append({\n",
    "            'Model': model,\n",
    "            'Prompt': prompt,\n",
    "            'Judge' : llm_eval_path.split('_')[1].replace('/', ''),  # Extract judge from path\n",
    "            'Same_Scores': count,\n",
    "            'Variance': variance\n",
    "        })\n",
    "\n",
    "        \n",
    "\n",
    "#print counters readable format\n",
    "print(\"\\nCounters in a readable format:\")\n",
    "for model, prompts in counters.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, count in prompts.items():\n",
    "        print(f\"  Prompt: {prompt}, Count: {count}\")\n",
    "\n",
    "# Print the variance scores\n",
    "print(\"\\nVariance scores:\")\n",
    "for model, prompts in scores_variance.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for prompt, variances in prompts.items():\n",
    "        if variances:\n",
    "            print(f\"  Prompt: {prompt}, Variance: {np.mean(variances):.4f}\")\n",
    "        else:\n",
    "            print(f\"  Prompt: {prompt}, Variance: No data\")\n",
    "\n",
    "\n",
    "\n",
    "report_df = pd.DataFrame(report_data)\n",
    "report_df.to_csv('csvFiles/report.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55314716",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564be9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#path to the scores\n",
    "gemini_path = \"csvFiles/final_scores_metrics_gemini.csv\"\n",
    "prometheus_path = \"csvFiles/final_scores_metrics_prometheus.csv\"\n",
    "#output file\n",
    "output_variance_path = \"output_folder/variance_by_group.jsonl\"\n",
    "\n",
    "\n",
    "def compute_group_variance(csv_path, source_label):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    #take the mean for every model\n",
    "    model_means = df.groupby(\"model\")[\"mean_score\"].mean()\n",
    "    model_variance = float(np.var(model_means))#compute the variance for the models\n",
    "\n",
    "    # take the mean for every prompt\n",
    "    prompt_means = df.groupby(\"prompt\")[\"mean_score\"].mean()\n",
    "    prompt_variance = float(np.var(prompt_means))#compute the variance for the prompts\n",
    "\n",
    "    return {\n",
    "        \"source\": source_label,\n",
    "        \"var_model\": round(model_variance, 6),\n",
    "        \"var_prompt\": round(prompt_variance, 6)\n",
    "    }\n",
    "\n",
    "#execute for both evaluators\n",
    "gemini_result = compute_group_variance(gemini_path, \"Gemini\")\n",
    "prometheus_result = compute_group_variance(prometheus_path, \"Prometheus\")\n",
    "\n",
    "#write the file in jsonl\n",
    "with open(output_variance_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(gemini_result) + \"\\n\")\n",
    "    f.write(json.dumps(prometheus_result) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved at: {output_variance_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
