{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd25f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateCSVwithSamples = False\n",
    "merge_csv_files = False\n",
    "llm_vs_manual = True\n",
    "\n",
    "\n",
    "# Read config json file\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "config_file = \"config.json\"\n",
    "if not os.path.exists(config_file):\n",
    "    print(f\"Config file not found: {config_file}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "models = config.get('models', [])\n",
    "prompts = config.get('prompts', [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9205cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_federico_path = \"manual_evaluate_translations_federico/\"\n",
    "folder_gianmarco_path = \"manual_evaluate_translations_gianmarco/\"\n",
    "\n",
    "if generateCSVwithSamples:\n",
    "    input_folder_path = \"translations/\"\n",
    "\n",
    "    os.makedirs(folder_federico_path, exist_ok=True)\n",
    "    os.makedirs(folder_gianmarco_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            input_path = os.path.join(input_folder_path, filename)\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Extract first 10 rows\n",
    "            first_10 = df.head(10)\n",
    "            output_federico = os.path.join(folder_federico_path, filename)\n",
    "            first_10.to_csv(output_federico, index=False)\n",
    "            \n",
    "            # Extract last 10 rows\n",
    "            last_10 = df.tail(10)\n",
    "            output_gianmarco = os.path.join(folder_gianmarco_path, filename)\n",
    "            last_10.to_csv(output_gianmarco, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82edd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged scored_cerbero_base.csv from both folders and saved to scores_manual/scored_cerbero_base.csv\n",
      "Merged scored_cerbero_detailed.csv from both folders and saved to scores_manual/scored_cerbero_detailed.csv\n",
      "Merged scored_cerbero_few_shot.csv from both folders and saved to scores_manual/scored_cerbero_few_shot.csv\n",
      "Merged scored_cerbero_role-based.csv from both folders and saved to scores_manual/scored_cerbero_role-based.csv\n",
      "Merged scored_cerbero_teacher_student.csv from both folders and saved to scores_manual/scored_cerbero_teacher_student.csv\n",
      "Merged scored_gemma_base.csv from both folders and saved to scores_manual/scored_gemma_base.csv\n",
      "Merged scored_gemma_detailed.csv from both folders and saved to scores_manual/scored_gemma_detailed.csv\n",
      "Merged scored_gemma_few_shot.csv from both folders and saved to scores_manual/scored_gemma_few_shot.csv\n",
      "Merged scored_gemma_role-based.csv from both folders and saved to scores_manual/scored_gemma_role-based.csv\n",
      "Merged scored_gemma_teacher_student.csv from both folders and saved to scores_manual/scored_gemma_teacher_student.csv\n",
      "Merged scored_llama3_base.csv from both folders and saved to scores_manual/scored_llama3_base.csv\n",
      "Merged scored_llama3_detailed.csv from both folders and saved to scores_manual/scored_llama3_detailed.csv\n",
      "Merged scored_llama3_few_shot.csv from both folders and saved to scores_manual/scored_llama3_few_shot.csv\n",
      "Merged scored_llama3_role-based.csv from both folders and saved to scores_manual/scored_llama3_role-based.csv\n",
      "Merged scored_llama3_teacher_student.csv from both folders and saved to scores_manual/scored_llama3_teacher_student.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge the csv files from both folders into a single CSV file\n",
    "def merge_csv_files(folder_path1, folder_path2):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for filename in os.listdir(folder_path1):\n",
    "        if filename.endswith('.csv'):\n",
    "            if filename in os.listdir(folder_path2):\n",
    "                path1 = os.path.join(folder_path1, filename)\n",
    "                path2 = os.path.join(folder_path2, filename)\n",
    "                \n",
    "                df1 = pd.read_csv(path1)\n",
    "                df2 = pd.read_csv(path2)\n",
    "                \n",
    "                # Merge the two dataframes\n",
    "                merged_df = pd.concat([merged_df, df1, df2], ignore_index=True)\n",
    "                # Save as csv\n",
    "                #new file name\n",
    "                filename = filename.replace('translation', 'scored')\n",
    "                output_path = os.path.join(\"scores_manual/\", filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                merged_df.to_csv(output_path, index=False)\n",
    "                print(f\"Merged {filename} from both folders and saved to {output_path}\")\n",
    "                # Clear the merged_df for the next file\n",
    "                merged_df = pd.DataFrame()\n",
    "\n",
    "if merge_csv_files:\n",
    "    # Merge the CSV files from both folders\n",
    "    merged_df = merge_csv_files(folder_federico_path, folder_gianmarco_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counters initialized: {'cerbero': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'gemma': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}, 'llama3': {'base': 0, 'detailed': 0, 'few_shot': 0, 'role-based': 0, 'teacher_student': 0}}\n"
     ]
    }
   ],
   "source": [
    "manual_eval_path = \"scores_manual/\"\n",
    "llm_eval_path =\"scores_prometheus/\"\n",
    "\n",
    "# dictiornary for each model and prompt in config there is a value counter, initialize that as 0\n",
    "counters = {model: {prompt: 0 for prompt in prompts} for model in models}\n",
    "\n",
    "print(\"Counters initialized:\", counters)\n",
    "\n",
    "if llm_vs_manual:\n",
    "    # Load the manual evaluation scores\n",
    "    manual_files = [f for f in os.listdir(manual_eval_path) if f.endswith('.csv')]\n",
    "    llm_files = [f for f in os.listdir(llm_eval_path) if f.endswith('.csv')]\n",
    "\n",
    "    for manual_file in manual_files:\n",
    "        if manual_file in llm_files:\n",
    "            manual_df = pd.read_csv(os.path.join(manual_eval_path, manual_file))\n",
    "            llm_df = pd.read_csv(os.path.join(llm_eval_path, manual_file))\n",
    "            # Compare the item in manual_df with llm_df\n",
    "            for index, row in manual_df.iterrows():\n",
    "                # Extract model and prompt from the filename (file format is scored_model_prompt.csv)\n",
    "                file = manual_file.replace('scored_', '').replace('.csv', '')\n",
    "                model, prompt = file.split('_', 1)\n",
    "                score_manual = row['Score']\n",
    "                \n",
    "                # Find the corresponding row in llm_df using the Sentence coulmn\n",
    "                llm_row = llm_df[llm_df['Sentence'] == row['Sentence']]\n",
    "                \n",
    "                if not llm_row.empty:\n",
    "                    score_llm = llm_row['Score'].values[0]\n",
    "                    # Compare scores\n",
    "                    if score_manual == score_llm:\n",
    "                        counters[model][prompt] += 1\n",
    "                        print(f\"Manual score {score_manual} is equal to LLM score {score_llm} for model {model} and prompt {prompt}\")\n",
    "\n",
    "\n",
    "\n",
    "# Print the final counters\n",
    "print(\"Final counters:\")\n",
    "for model, prompts in counters.items():\n",
    "    for prompt, count in prompts.items():\n",
    "        print(f\"Model: {model}, Prompt: {prompt}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
