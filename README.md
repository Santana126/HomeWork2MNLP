# Archaic Italian to Modern Italian Translation & LLM Evaluation

## Homework Summary

This Homework addresses the translation of archaic Italian sentences (13th century) into modern Italian using various Large Language Models (LLMs). Beyond simply performing translations, a core focus is on comprehensively evaluating the quality of these translations and critically assessing the reliability of LLMs when used as automated judges. We compare the performance of different LLMs and prompting strategies for translation, and then benchmark two LLM-based evaluation approaches (Prometheus and Gemini) against human manual assessments to determine their agreement and effectiveness.

## Key Concepts

*   **Translation Task:** Converting 13th-century archaic Italian text (from `dataset_cleaned.csv`) to modern Italian.
*   **LLM Comparison:** Evaluating translation quality across different models available via Ollama (Cerbero, Gemma, Llama3).
*   **Prompting Strategy Impact:** Investigating how various prompt designs (`base`, `detailed`, `few_shot`, `role-based`, `teacher_student`) influence translation output.
*   **LLM-as-a-Judge Evaluation:**
    *   **Prometheus:** Utilizing `Unbabel/M-Prometheus-3B` for automated quality scoring.
    *   **Gemini:** Employing Google's Gemini API with a detailed rubric for an alternative automated judgment.
*   **Human Benchmark:** Conducting manual evaluations on a subset of translations to establish ground truth scores.
*   **Judge Agreement Analysis:** Quantifying the agreement and variance between automated (Prometheus and Gemini) and human scores to assess the reliability of LLM judges.

## Codebase Overview

The project is structured around three main Jupyter notebooks, each handling a distinct phase of the workflow:

*   **`main.ipynb`**:
    *   **Function:** Manages the core translation process. It iterates through the dataset, applies various prompting strategies to configured LLMs (Cerbero, Gemma, Llama3 via Ollama), and saves the generated translations.
    *   **Evaluation:** Initiates the first phase of automated evaluation using `Unbabel/M-Prometheus-3B` to score the translations, saving these scores for further analysis.
    *   **Output:** Translations are saved in `translations/`, and Prometheus scores in `scores_prometheus/`.

*   **`judge.ipynb`**:
    *   **Function:** Implements LLM-as-a-Judge evaluation using Google's Gemini API. It loads translations from `main.ipynb`'s output and applies a detailed, predefined rubric to assign quality scores.
    *   **Output:** Gemini-based scores are saved in `scores_gemini/`.

*   **`manual_eval.ipynb`**:
    *   **Function:** Facilitates the manual evaluation process by preparing and merging human-scored translation samples.
    *   **Analysis:** Crucially, this notebook compares the scores generated by the Prometheus LLM judge (`scores_prometheus/`) with the human manual scores (`scores_manual/`). It calculates metrics such as `Same_Scores` (count of matching scores) and `Variance` (mean squared difference) to quantify agreement.
    *   **Output:** Generates `report.csv`, summarizing the inter-judge agreement.

## Evaluation Insights (from `report.csv`)

The `report.csv` file provides a comparative summary of how well the Prometheus LLM judge's scores align with human manual evaluations for each translation model and prompting strategy. Key observations typically include:

*   **Consistency:** Certain models (e.g., `llama3`) tend to exhibit higher agreement with human judgments and lower score variance across various prompts.
*   **Prompt Effectiveness:** Specific prompting strategies (e.g., `few_shot`, `teacher_student`) can significantly improve the alignment between automated and human evaluations for particular LLMs.

## Requirements

*   **Python Environment:** Python 3.8+ with standard data science libraries (`pandas`, `tqdm`).
*   **LLM Dependencies:** `transformers`, `accelerate`, `torch`, `ollama` client library, `google-generativeai` client library.
*   **Ollama:** Ensure the Ollama server is running locally with `galatolo/cerbero-7b`, `gemma`, and `llama3` models pulled.
*   **Gemini API Key:** A valid Google Gemini API key is required, configured in a `gemini_config.ini` file.

## Comprehensive Report

For a detailed exposition of the project's methodology, experimental setup, in-depth results, discussions, and conclusions, please refer to the accompanying **PDF Report**.

---