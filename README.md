# Archaic Italian to Modern Italian Translation & LLM Evaluation

## Homework Summary

This Homework addresses the translation of archaic Italian sentences (13th century) into modern Italian using various Large Language Models (LLMs). Beyond simply performing translations, a core focus is on comprehensively evaluating the quality of these translations and critically assessing the reliability of LLMs when used as automated judges. We compare the performance of different LLMs and prompting strategies for translation, and then benchmark two LLM-based evaluation approaches (Prometheus and Gemini) against human manual assessments to determine their agreement and effectiveness.

## Key Concepts

*   **Translation Task:** Converting 13th-century archaic Italian text (from `dataset/dataset_cleaned.csv`) to modern Italian.
*   **LLM Comparison:** Evaluating translation quality across different models available via Ollama (e.g., Cerbero, Gemma, Llama3).
*   **Prompting Strategy Impact:** Investigating how various prompt designs (`base`, `detailed`, `few_shot`, `role-based`, `teacher_student`) influence translation output.
*   **LLM-as-a-Judge Evaluation:**
    *   **Gemini:** Employing Google's Gemini API with a detailed rubric for automated quality scoring.
    *   **Prometheus:** Utilizing `Unbabel/M-Prometheus-3B` for an alternative automated quality scoring.
*   **Human Benchmark:** Conducting manual evaluations on a subset of translations to establish ground truth scores.
*   **Judge Agreement Analysis:** Quantifying the agreement and variance between automated (Gemini, Prometheus) and human scores to assess the reliability of LLM judges.

## Codebase Overview

The project is structured around four main Jupyter notebooks, each handling a distinct phase of the workflow:

*   **`main.ipynb`**:
    *   **Function:** Manages the core translation process. It loads the dataset (`dataset/dataset_cleaned.csv`), iterates through a configured list of LLMs (via Ollama) and prompting strategies, and generates the translations.
    *   **Output:** The resulting translations are saved as individual CSV files in the `csvFiles/translations/` directory, with each file named according to the model and prompt used (e.g., `translation_llama3_few_shot.csv`).

*   **`judge.ipynb`**:
    *   **Function:** This notebook is dedicated to the **automated evaluation** of the translations generated by `main.ipynb`. It implements two distinct "LLM-as-a-Judge" pipelines:
        1.  **Gemini Judge:** Uses the Google Gemini API with a detailed, predefined rubric to score each translation on a 1-5 scale.
        2.  **Prometheus Judge:** Uses the locally-hosted `Unbabel/M-Prometheus-3B` model to perform the same scoring task.
    *   **Output:** Scores from both judges are saved in separate directories: `csvFiles/scores_gemini/` and `csvFiles/scores_prometheus/`.

*   **`manual_eval.ipynb`**:
    *   **Function:** This notebook orchestrates the final analysis and comparison. It includes logic to:
        1.  Prepare and merge human-scored translation samples. The manual scores are expected to be in `csvFiles/scores_manual/`.
        2.  Compare the scores from the automated judges (Gemini and Prometheus) against the human (manual) scores.
        3.  Calculate metrics like score agreement counts and Mean Squared Error to quantify how well each LLM judge aligns with human evaluators.
    *   **Output:** Generates a final `report.csv` summarizing the inter-judge agreement and a `variance_by_group.jsonl` file with higher-level variance metrics.

*   **`utils.ipynb`**:
    *   **Function:** A utility notebook containing helper functions, primarily for converting the project's CSV files into JSONL format for easier data interchange or use with other tools. It also contains helper scripts for file management tasks like renaming.

## Evaluation Insights (from `report.csv`)

The `report.csv` file provides a comparative summary of how well the automated LLM judges' scores align with human manual evaluations for each translation model and prompting strategy. Key observations typically include:

*   **Judge Reliability:** Which LLM judge (Gemini or Prometheus) shows a higher agreement and lower Mean Squared Error when compared to human scores.
*   **Model Consistency:** Certain translation models (e.g., `llama3`) may exhibit higher agreement with human judgments across various prompts.
*   **Prompt Effectiveness:** Specific prompting strategies (e.g., `few_shot`, `teacher_student`) can significantly improve the quality of translations, which is then reflected in higher scores from all evaluators.

## Requirements

*   **Python Environment:** Python 3.8+ with standard data science libraries (`pandas`, `tqdm`).
*   **LLM Dependencies:** `transformers`, `accelerate`, `torch`, `ollama` client library, `google-generativeai` client library.
*   **Ollama:** Ensure the Ollama server is running locally with the required models pulled (e.g., `galatolo/cerbero-7b`, `gemma`, `llama3`).
*   **Prometheus Model:** The `Unbabel/M-Prometheus-3B` model must be downloaded for local evaluation.
*   **Gemini API Key:** A valid Google Gemini API key is required, configured in a `gemini_config.ini` file.

## Comprehensive Report

For a detailed exposition of the project's methodology, experimental setup, in-depth results, discussions, and conclusions, please refer to the accompanying **PDF Report**.