{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e68473",
   "metadata": {},
   "source": [
    "# With Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa64",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"dataset/dataset_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path) #creation of dataframe\n",
    "output_path = \"dataset/output.csv\"\n",
    "col_name = \"Sentence\"\n",
    "models = [\"gemma\", \"mistral\"]\n",
    "#prompts\n",
    "prompt_templates={\n",
    "    \"base\":        \"Translate the following sentence from archaic italian to modern italian:\\n\\n{sentence}\\n\\nTranslation:\",\n",
    "    \"detailed\": \"The following text is written in archaic Italian from the 13th century, originating from the Tuscany region. Rewrite it in modern Italian while preserving the original meaning, clarity, and syntactic coherence. First analyze the structure, then identify key words, then write the final version:\\n\\n {sentence}\\n\\nTranslation:\",\n",
    "    \"role-based\":   \"You are an expert linguist specializing in the evolution of Italian language. Translate this 13th century Tuscan text to contemporary Italian while preserving the original meaning, clarity, and syntactic coherence: \\n\\n{sentence}\\n\\nTranslation:\",\n",
    "    \"few_shot\": (\n",
    "            \"Here are some examples of sentences in archaic Italian from the 13th century translated into modern Italian:\\n\\n\"\n",
    "            \"Archaic Italian: «quella guerra ben fatta l' opera perché etc.». Modern Italian: «quella guerra fu condotta bene, e l'opera fu compiuta come previsto.».\\n\"\n",
    "            \"Archaic Italian: «crudele, e di tutte le colpe pigli vendetta». Modern Italian: «crudele, e si vendica di tutte le colpe.»\\n\"\n",
    "            \"Archaic Italian: «Non d' altra forza d' animo fue ornato Ponzio Aufidiano». Modern Italian: «Ponzio Aufidiano non era dotato di altro vigore d’animo.»\\n\\n\"\n",
    "            \"Now translate the following sentence from archaic italian to modern italian while preserving the original meaning:\\n\\n\"\n",
    "            \"{sentence}\\n\\nTranslation:\"\n",
    "                ),\n",
    "    \"teacher_student\": (\n",
    "            \"A student asked: 'What does this old Italian sentence mean in modern language?'\\n\"\n",
    "            \"You, a university professor of historical linguistics, respond with a clear and faithful modern Italian translation\\n\\n{sentence}\\n\\nTranslation:\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb4dde",
   "metadata": {},
   "source": [
    "## Translate the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b10ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Translation with model: gemma | prompt: base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [05:03<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_gemma_base.csv'\n",
      "\n",
      " Translation with model: gemma | prompt: detailed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [23:56<00:00, 14.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_gemma_detailed.csv'\n",
      "\n",
      " Translation with model: gemma | prompt: role-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [06:27<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_gemma_role-based.csv'\n",
      "\n",
      " Translation with model: gemma | prompt: few_shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [05:59<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_gemma_few_shot.csv'\n",
      "\n",
      " Translation with model: gemma | prompt: teacher_student\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [09:28<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_gemma_teacher_student.csv'\n",
      "\n",
      " Translation with model: mistral | prompt: base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [17:32<00:00, 10.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_mistral_base.csv'\n",
      "\n",
      " Translation with model: mistral | prompt: detailed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [12:58<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_mistral_detailed.csv'\n",
      "\n",
      " Translation with model: mistral | prompt: role-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [08:00<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_mistral_role-based.csv'\n",
      "\n",
      " Translation with model: mistral | prompt: few_shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [04:23<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_mistral_few_shot.csv'\n",
      "\n",
      " Translation with model: mistral | prompt: teacher_student\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [06:02<00:00,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_mistral_teacher_student.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')#client to local ollama\n",
    "\n",
    "for model_name in models:#iterate through models\n",
    "    for prompt_name, prompt_template in prompt_templates.items():#iterate through prompt templates\n",
    "        print(f\"\\n Translation with model: {model_name} | prompt: {prompt_name}\")\n",
    "        translations = []#list to store translations\n",
    "        for sentence in tqdm(df[col_name]):#iterate through sentences\n",
    "            try:\n",
    "                prompt = prompt_template.format(sentence=sentence)#give the prompt\n",
    "\n",
    "                response = client.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )#ollama api call\n",
    "\n",
    "                translation = response['message']['content'].strip()#extract the translation\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                translation = f\"[ERROR]: {e}\"\n",
    "            translations.append(translation)#append the translation to the list\n",
    "\n",
    "        df[\"translation\"] = translations# add the translations to the dataframe\n",
    "        if model_name == \"qwen:7B\":\n",
    "            output_file = f\"translation_qwen_{prompt_name}.csv\"# create the output file name\n",
    "        else:\n",
    "            output_file = f\"translation_{model_name}_{prompt_name}.csv\"# create the output file name\n",
    "\n",
    "        df.to_csv(output_file, index=False)# save the dataframe to a csv file\n",
    "        print(f\"Translation saved in '{output_file}'\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1068106edd6e4eb7a9a608d4afebb3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\feder\\Documents\\GitHub\\HomeWork2MNLP\\models\\m_prometheus_3b\\models--Unbabel--M-Prometheus-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4102905aa2d24dc6ba6c5d0a7755cf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f635123ce14228956beeabf8b8ad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a566c017284e88a2d35fdcecd505b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f330bbd4804497abd910dabe41b2213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf85dd0482b41469630a72831afbc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521a523fe95c42399cbf79c5354b37ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def490e87e7a4456b8a8c2d8faaf86d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9f0e505c0d4e9ba607ae626b8fdb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8347ec276a4732a252eec035cffd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30491692a967421eb08abeeedfff7b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "cache_dir = \"./models/m_prometheus_3b\"  # Optional: specify local directory to cache the model\n",
    "\n",
    "# Load tokenizer and model, this will download if not cached locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b366ffb",
   "metadata": {},
   "source": [
    "LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1.  **Completely Unacceptable Translation:**\n",
    "    *   The translation has no pertinence with the original meaning.\n",
    "    *   The generated sentence is either gibberish, makes no sense, or is completely unrelated to the archaic text.\n",
    "    *   Contains severe errors that render it incomprehensible or entirely misleading.\n",
    "\n",
    "2.  **Severe Semantic Errors/Omissions:**\n",
    "    *   The translation contains significant semantic errors, critical omissions of meaning from the archaic text, or substantial incorrect additions.\n",
    "    *   While some words might be recognizable, the core meaning is lost or heavily distorted.\n",
    "    *   The modernization is poor, leaving many archaic forms or incorrectly modernizing them.\n",
    "    *   Likely many grammatical errors in modern Italian.\n",
    "\n",
    "3.  **Partially Wrong Translation / Lackluster:**\n",
    "    *   The translation captures some of the original meaning but is lackluster or contains noticeable errors.\n",
    "    *   Errors are mostly minor (e.g., awkward phrasing, typos, minor grammatical mistakes in modern Italian, some less critical semantic misunderstandings or misinterpretations of archaic terms).\n",
    "    *   Some archaic features might be awkwardly modernized or missed.\n",
    "    *   The overall quality is mediocre; it's understandable but clearly flawed.\n",
    "\n",
    "4.  **Good Translation:**\n",
    "    *   The translation is mostly accurate and successfully conveys the core meaning of the archaic sentence.\n",
    "    *   It is substantially faithful to the original text.\n",
    "    *   The modern Italian is fluent and comprehensible.\n",
    "    *   Archaic features are generally well modernized.\n",
    "    *   There might be minor stylistic imperfections (e.g., style doesn't perfectly match natural modern Italian, slight awkwardness) or very minor errors that do not significantly impact understanding or meaning.\n",
    "\n",
    "5.  **Perfect Translation:**\n",
    "    *   The translation is completely accurate, fully conveying the meaning and nuances of the original archaic sentence.\n",
    "    *   It is perfectly fluent, natural-sounding, and grammatically correct modern Italian.\n",
    "    *   All archaic linguistic features (vocabulary, syntax, orthography) are correctly and appropriately modernized.\n",
    "    *   The style is appropriate for modern Italian.\n",
    "    *   No errors.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric Defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed498a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(archaic_text, modern_translation, rubric):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "                Your task is to assess the quality of the \"Candidate Modern Translation\" provided below, based on the \"Original Archaic Sentence\".\n",
    "\n",
    "                Please use the following detailed 1-5 scale and rubric for your evaluation:\n",
    "                --- RUBRIC START ---\n",
    "                {rubric}\n",
    "                --- RUBRIC END ---\n",
    "\n",
    "                Original Archaic Sentence:\n",
    "                \"{archaic_text}\"\n",
    "\n",
    "                Candidate Modern Translation:\n",
    "                \"{modern_translation}\"\n",
    "\n",
    "                Carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "                Output ONLY the integer score. Do not add any explanation, prefix, or other text.\n",
    "\n",
    "                Score:\"\"\"\n",
    "\n",
    "print(\"Judge Prompting Function Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re # For parsing the score\n",
    "\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "JUDGE_MODEL_NAME = \"gemini-pro\"\n",
    "\n",
    "\n",
    "\n",
    "llms = []\n",
    "prompts = []\n",
    "\n",
    "for llm in llms:\n",
    "\n",
    "    for prompt in prompts:\n",
    "\n",
    "        CSV_FILE_PATH = \"translations.csv\"\n",
    "        OUTPUT_CSV_PATH = \"judged_translations.csv\"\n",
    "\n",
    "\n",
    "        TRANSLATION_SYSTEM_COLUMNS = [\"translation_system_1\", \"translation_system_2\", \"translation_system_3\"]\n",
    "        ARCHAIC_SENTENCE_COLUMN = \"archaic_sentence\"\n",
    "\n",
    "        print(\"Setup Complete.\")\n",
    "        print(\"\\nJudge Model:\", JUDGE_MODEL_NAME)\n",
    "        print(\"\\nJudging: \",) # The model and prompt that is judging\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a09461",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_translations = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"Successfully loaded {len(df_translations)} rows from {CSV_FILE_PATH}\")\n",
    "    # Display first few rows to verify\n",
    "    print(\"\\nFirst 5 rows of your data:\")\n",
    "    print(df_translations.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file {CSV_FILE_PATH} was not found. Please check the path.\")\n",
    "    df_translations = pd.DataFrame() # Create empty df to avoid later errors if notebook run continues\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    df_translations = pd.DataFrame()\n",
    "\n",
    "# List to store all judgment results\n",
    "all_judgments_data = []\n",
    "\n",
    "\n",
    "# Initialize the generative model\n",
    "judge_model = genai.GenerativeModel(JUDGE_MODEL_NAME)\n",
    "print(f\"Using Judge Model: {judge_model.model_name}\")\n",
    "\n",
    "# Safety setting for generation - can be adjusted.\n",
    "# Higher values for 'block_...' mean more conservative blocking.\n",
    "# See https://ai.google.dev/docs/safety_setting_gemini\n",
    "# You might want to adjust these if your content is being blocked.\n",
    "# For this task, default should be mostly fine.\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    # temperature=0.1 # For more deterministic output from the judge\n",
    ")\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "\n",
    "if not df_translations.empty:\n",
    "    print(f\"\\nStarting judging process for {len(df_translations)} archaic sentences...\")\n",
    "    for index, row in df_translations.iterrows():\n",
    "        archaic_sent = str(row[ARCHAIC_SENTENCE_COLUMN]) # Ensure it's a string\n",
    "        print(f\"\\nJudging translations for archaic sentence {index + 1}/{len(df_translations)}: \\\"{archaic_sent[:70]}...\\\"\")\n",
    "\n",
    "        for system_col_name in TRANSLATION_SYSTEM_COLUMNS:\n",
    "            modern_trans = str(row[system_col_name]) if pd.notna(row[system_col_name]) else \"\" # Handle potential NaN/empty translations\n",
    "\n",
    "            if not modern_trans: # Skip if no translation from this system\n",
    "                print(f\"  - System '{system_col_name}': No translation provided. Skipping.\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"NO_TRANSLATION_PROVIDED\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": \"No translation provided by system\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            prompt_text = create_judge_prompt(archaic_sent, modern_trans, JUDGE_RUBRIC)\n",
    "\n",
    "            try:\n",
    "                print(f\"  - System '{system_col_name}': Sending to judge...\")\n",
    "                response = judge_model.generate_content(\n",
    "                    prompt_text,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings\n",
    "                )\n",
    "                raw_score_text = response.text.strip()\n",
    "                print(f\"    Raw response from Judge: '{raw_score_text}'\")\n",
    "\n",
    "                # Attempt to parse the score\n",
    "                parsed_score = None\n",
    "                error_msg = None\n",
    "                match = re.search(r'^\\s*([1-5])\\s*$', raw_score_text) # Looks for a single digit 1-5, allows whitespace\n",
    "                if match:\n",
    "                    parsed_score = int(match.group(1))\n",
    "                else:\n",
    "                    # Fallback if LLM didn't follow instructions perfectly\n",
    "                    fallback_match = re.search(r'\\b([1-5])\\b', raw_score_text) # Finds a digit 1-5 within other text\n",
    "                    if fallback_match:\n",
    "                        parsed_score = int(fallback_match.group(1))\n",
    "                        print(f\"    Warning: Parsed score '{parsed_score}' from less strict match. LLM Response: '{raw_score_text}'\")\n",
    "                    else:\n",
    "                        error_msg = f\"Could not parse a 1-5 score from: '{raw_score_text}'\"\n",
    "                        print(f\"    ERROR: {error_msg}\")\n",
    "\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": raw_score_text,\n",
    "                    \"judge_llm_score_parsed\": parsed_score,\n",
    "                    \"error_message\": error_msg\n",
    "                })\n",
    "\n",
    "            except genai.types.generation_types.BlockedPromptException as bpe:\n",
    "                print(f\"    ERROR: Prompt for '{system_col_name}' was blocked. Reason: {bpe}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"BLOCKED_PROMPT\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": f\"Prompt blocked by API: {bpe}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR judging translation from '{system_col_name}': {e}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"API_ERROR\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": str(e)\n",
    "                })\n",
    "\n",
    "            # Be mindful of API rate limits (requests per minute)\n",
    "            # Gemini free tier allows 60 RPM for gemini-pro.\n",
    "            # If you have many items or use a model with stricter limits, uncomment and adjust sleep time.\n",
    "            # time.sleep(1.1) # Sleep for a bit over 1 second to stay under 60 RPM\n",
    "\n",
    "    print(\"\\nJudging process complete.\")\n",
    "else:\n",
    "    print(\"No data loaded from CSV. Skipping judging process.\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_judged_results = pd.DataFrame(all_judgments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3096571",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_judged_results.empty:\n",
    "    print(\"\\n--- Judged Results (First 10 rows) ---\")\n",
    "    print(df_judged_results.head(10))\n",
    "\n",
    "    print(\"\\n--- Value Counts of Parsed Scores ---\")\n",
    "    print(df_judged_results['judge_llm_score_parsed'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    print(\"\\n--- Cases with Parsing Errors or API Issues ---\")\n",
    "    print(df_judged_results[df_judged_results['judge_llm_score_parsed'].isna() & (df_judged_results['error_message'] != \"No translation provided by system\")])\n",
    "\n",
    "\n",
    "    # Save the results to a new CSV\n",
    "    try:\n",
    "        df_judged_results.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "        print(f\"\\nSuccessfully saved judged results to {OUTPUT_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No judgments were made. Output DataFrame is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
