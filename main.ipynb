{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abe49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errori nelle traduzioni da fixare\n",
    "\"\"\"\n",
    "    gemma detailed : fornisce anche la spiegazione della risposta\n",
    "    llama3 tutti: fornisce anche la spiegazione nella risposta, ancora piu dettagliata in detailed and role_based\n",
    "\n",
    "\"\"\"\n",
    "#####\n",
    "# Note \n",
    "\"\"\"\n",
    "    Cerb detailed e role based fa una traduzione in inglese (scrivere nel report se persiste)\n",
    "    Cerb teacher stud alcune inglese e alcune non solo traduzione \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1c7d8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# open the config file \n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    models = config.get('models', [])\n",
    "    prompts = config.get('prompts', [])\n",
    "\n",
    "    print(\"Config success:\")\n",
    "    print(f\"Models: {models}\")\n",
    "    print(f\"Prompts: {prompts}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Config file '{CONFIG_FILE_PATH}' not found.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: File '{CONFIG_FILE_PATH}' not a valid JSON.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa64",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"dataset/dataset_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path) #creation of dataframe\n",
    "output_path = \"dataset/output.csv\"\n",
    "col_name = \"Sentence\"\n",
    "# models loaded from config file\n",
    "#prompts\n",
    "prompt_templates={\n",
    "    \"base\":        \"Translate the following sentence from archaic italian to modern italian:\\n\\n{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"detailed\": \"The following text is written in archaic Italian from the 13th century, originating from the Tuscany region. Rewrite it in modern Italian while preserving the original meaning, clarity, and syntactic coherence. First analyze the structure, then identify key words, then write the final version:\\n\\n {sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"role-based\":   \"You are an expert linguist specializing in the evolution of Italian language. Translate this 13th century Tuscan text to contemporary Italian while preserving the original meaning, clarity, and syntactic coherence: \\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"few_shot\": (\n",
    "            \"Here are some examples of sentences in archaic Italian from the 13th century translated into modern Italian:\\n\\n\"\n",
    "            \"Archaic Italian: «quella guerra ben fatta l' opera perché etc.». Modern Italian: «quella guerra fu condotta bene, e l'opera fu compiuta come previsto.».\\n\"\n",
    "            \"Archaic Italian: «crudele, e di tutte le colpe pigli vendetta». Modern Italian: «crudele, e si vendica di tutte le colpe.»\\n\"\n",
    "            \"Archaic Italian: «Non d' altra forza d' animo fue ornato Ponzio Aufidiano». Modern Italian: «Ponzio Aufidiano non era dotato di altro vigore d’animo.»\\n\\n\"\n",
    "            \"Now translate the following sentence from archaic italian to modern italian while preserving the original meaning:\\n\\n\"\n",
    "            \"{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\"\n",
    "                ),\n",
    "    \"teacher_student\": (\n",
    "            \"A student asked: 'What does this old Italian sentence mean in modern language?'\\n\"\n",
    "            \"You, a university professor of historical linguistics, respond with a clear and faithful modern Italian translation\\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb4dde",
   "metadata": {},
   "source": [
    "## Translate the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')#client to local ollama\n",
    "\n",
    "for model_name in models:#iterate through models\n",
    "    for prompt_name, prompt_template in prompt_templates.items():#iterate through prompt templates\n",
    "        print(f\"\\n Translation with model: {model_name} | prompt: {prompt_name}\")\n",
    "        translations = []#list to store translations\n",
    "        for sentence in tqdm(df[col_name]):#iterate through sentences\n",
    "            try:\n",
    "                prompt = prompt_template.format(sentence=sentence)#give the prompt\n",
    "\n",
    "                response = client.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )#ollama api call\n",
    "\n",
    "                translation = response['message']['content'].strip()#extract the translation\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                translation = f\"[ERROR]: {e}\"\n",
    "            translations.append(translation)#append the translation to the list\n",
    "\n",
    "        df[\"translation\"] = translations# add the translations to the dataframe\n",
    "        # TODO\n",
    "        if model_name == \"qwen:7B\":\n",
    "            output_file = f\"translations/translation_qwen_{prompt_name}.csv\"# create the output file name\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            output_file = f\"translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            output_file = f\"translations/translation_{model_name}_{prompt_name}.csv\"# create the output file name\n",
    "\n",
    "        df.to_csv(output_file, index=False)# save the dataframe to a csv file\n",
    "        print(f\"Translation saved in '{output_file}'\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff459cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
