{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e68473",
   "metadata": {},
   "source": [
    "# With Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa64",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"dataset/dataset_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path) #creation of dataframe\n",
    "output_path = \"dataset/output.csv\"\n",
    "col_name = \"Sentence\"\n",
    "models = [\"galatolo/cerbero-7b\",\"llama3\"]\n",
    "#prompts\n",
    "prompt_templates={\n",
    "    \"base\":        \"Translate the following sentence from archaic italian to modern italian:\\n\\n{sentence}\\n\\n.Provide only the translated sentence:\",\n",
    "    \"detailed\": \"The following text is written in archaic Italian from the 13th century, originating from the Tuscany region. Rewrite it in modern Italian while preserving the original meaning, clarity, and syntactic coherence. First analyze the structure, then identify key words, then write the final version:\\n\\n {sentence}\\n\\nProvide only the translated sentence:\",\n",
    "    \"role-based\":   \"You are an expert linguist specializing in the evolution of Italian language. Translate this 13th century Tuscan text to contemporary Italian while preserving the original meaning, clarity, and syntactic coherence: \\n\\n{sentence}\\n\\nProvide only the translated sentence:\",\n",
    "    \"few_shot\": (\n",
    "            \"Here are some examples of sentences in archaic Italian from the 13th century translated into modern Italian:\\n\\n\"\n",
    "            \"Archaic Italian: «quella guerra ben fatta l' opera perché etc.». Modern Italian: «quella guerra fu condotta bene, e l'opera fu compiuta come previsto.».\\n\"\n",
    "            \"Archaic Italian: «crudele, e di tutte le colpe pigli vendetta». Modern Italian: «crudele, e si vendica di tutte le colpe.»\\n\"\n",
    "            \"Archaic Italian: «Non d' altra forza d' animo fue ornato Ponzio Aufidiano». Modern Italian: «Ponzio Aufidiano non era dotato di altro vigore d’animo.»\\n\\n\"\n",
    "            \"Now translate the following sentence from archaic italian to modern italian while preserving the original meaning:\\n\\n\"\n",
    "            \"{sentence}\\n\\nProvide only the translated sentence:\"\n",
    "                ),\n",
    "    \"teacher_student\": (\n",
    "            \"A student asked: 'What does this old Italian sentence mean in modern language?'\\n\"\n",
    "            \"You, a university professor of historical linguistics, respond with a clear and faithful modern Italian translation\\n\\n{sentence}\\n\\nProvide only the translated sentence:\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb4dde",
   "metadata": {},
   "source": [
    "## Translate the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b10ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Translation with model: galatolo/cerbero-7b | prompt: base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [15:59<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_cerbero_base.csv'\n",
      "\n",
      " Translation with model: galatolo/cerbero-7b | prompt: detailed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [18:35<00:00, 11.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_cerbero_detailed.csv'\n",
      "\n",
      " Translation with model: galatolo/cerbero-7b | prompt: role-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [19:56<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_cerbero_role-based.csv'\n",
      "\n",
      " Translation with model: galatolo/cerbero-7b | prompt: few_shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [23:41<00:00, 14.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_cerbero_few_shot.csv'\n",
      "\n",
      " Translation with model: galatolo/cerbero-7b | prompt: teacher_student\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [20:28<00:00, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation saved in 'translation_cerbero_teacher_student.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')#client to local ollama\n",
    "\n",
    "for model_name in models:#iterate through models\n",
    "    for prompt_name, prompt_template in prompt_templates.items():#iterate through prompt templates\n",
    "        print(f\"\\n Translation with model: {model_name} | prompt: {prompt_name}\")\n",
    "        translations = []#list to store translations\n",
    "        for sentence in tqdm(df[col_name]):#iterate through sentences\n",
    "            try:\n",
    "                prompt = prompt_template.format(sentence=sentence)#give the prompt\n",
    "\n",
    "                response = client.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )#ollama api call\n",
    "\n",
    "                translation = response['message']['content'].strip()#extract the translation\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                translation = f\"[ERROR]: {e}\"\n",
    "            translations.append(translation)#append the translation to the list\n",
    "\n",
    "        df[\"translation\"] = translations# add the translations to the dataframe\n",
    "        if model_name == \"qwen:7B\":\n",
    "            output_file = f\"translation_qwen_{prompt_name}.csv\"# create the output file name\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            output_file = f\"translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            output_file = f\"translation_{model_name}_{prompt_name}.csv\"# create the output file name\n",
    "\n",
    "        df.to_csv(output_file, index=False)# save the dataframe to a csv file\n",
    "        print(f\"Translation saved in '{output_file}'\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59461769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6411beb6eb4624b0fac6751823c2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "cache_dir = \"./models/m_prometheus_3b\"  #local directory to cache the model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = (\n",
    "    \"Evaluate the following translation from archaic italian to modern italian and give a score from 1 to 5 according to the following definitions. \"\n",
    "    \"1. Completely unacceptable translation: the translation has no pertinence with the original meaning, the generated sentence is either gibberish or something that makes no sense.\\n\"\n",
    "    \"2. Severe semantic errors, omissions or substantial add ons on the original sentence. The errors are of semantic and syntactic nature. It’s still something no human would ever write.\\n\"\n",
    "    \"3. Partially wrong translation, the translation is lackluster, it contains errors, but are mostly minor errors, like typos, or small semantic errors.\\n\"\n",
    "    \"4. Good translation. The translation is mostly right, substantially faithful to the original text, but the style does not perfectly match the original sentence, still fluent and comprehensible, and could be semantically acceptable.\\n\"\n",
    "    \"5. Perfect translation. The translation is accurate, fluent, complete and coherent. It retained the original meaning as much as it could.\\n\\n\"\n",
    "    \"Original text:\\n{original}\\n\\n\"\n",
    "    \"Translation:\\n{translation}\\n\\n\"\n",
    "    \"Score:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall numpy -y && pip install numpy==1.24.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f3f6c",
   "metadata": {},
   "source": [
    "## Evaluation using pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412e0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      " 10%|█         | 10/97 [03:40<30:59, 21.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 97/97 [33:55<00:00, 20.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_galatolo/cerbero-7b_base.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [34:29<00:00, 21.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_galatolo/cerbero-7b_detailed.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [35:17<00:00, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_galatolo/cerbero-7b_role-based.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [35:23<00:00, 21.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_galatolo/cerbero-7b_few_shot.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [32:42<00:00, 20.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_galatolo/cerbero-7b_teacher_student.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [34:33<00:00, 21.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_llama3_base.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [36:19<00:00, 22.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_llama3_detailed.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [36:35<00:00, 22.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_llama3_role-based.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [33:52<00:00, 20.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_llama3_few_shot.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\feder\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [32:01<00:00, 19.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi salvati in 'scored_llama3_teacher_student.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "# Inizializza un pipeline con il modello M-Prometheus\n",
    "from transformers import TextGenerationPipeline  # Usa GPU se disponibile, altrimenti CPU\n",
    "judge_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Funzione per estrarre punteggio numerico da output del modello\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        score = int(output_text.strip().split()[0])\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "    except:\n",
    "        pass\n",
    "    return \"[INVALID OUTPUT]\"\n",
    "\n",
    "# Valutazione\n",
    "for model_name in models:\n",
    "    for prompt_name in prompt_templates:\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"translation_qwen_{prompt_name}.csv\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            file_path = f\"translation_{model_name}_{prompt_name}.csv\"\n",
    "        df_translation = pd.read_csv(file_path)\n",
    "\n",
    "        scores = []\n",
    "        for _, row in tqdm(df_translation.iterrows(), total=len(df_translation)):\n",
    "            original = row[col_name]\n",
    "            translation = row[\"translation\"]\n",
    "\n",
    "            prompt = evaluation_prompt_template.format(original=original, translation=translation)\n",
    "\n",
    "            output = judge_pipeline(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]\n",
    "            score = extract_score(output[len(prompt):])  # prendi solo la parte generata\n",
    "            scores.append(score)\n",
    "\n",
    "        df_translation[\"score\"] = scores\n",
    "        if model_name == \"qwen:7B\":\n",
    "            df_translation.to_csv(f\"scored_qwen_{prompt_name}.csv\", index=False)\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            df_translation.to_csv(f\"scored_cerbero_{prompt_name}.csv\", index=False)\n",
    "        else:\n",
    "            df_translation.to_csv(f\"scored_{model_name}_{prompt_name}.csv\", index=False)\n",
    "        print(f\"Punteggi salvati in 'scored_{model_name}_{prompt_name}.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fcafa",
   "metadata": {},
   "source": [
    "## Evaluation using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "batch_size = 16  # Numero di richieste per batch\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        score = int(output_text.strip().split()[0])\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "    except:\n",
    "        return \"[INVALID OUTPUT]\"\n",
    "\n",
    "# Valutazione su tutti i file\n",
    "for model_name in models:\n",
    "    for prompt_name in prompt_templates:\n",
    "        file_path = f\"translation_qwen_{prompt_name}.csv\" if model_name == \"qwen:7B\" else f\"translation_{model_name}_{prompt_name}.csv\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        prompts = [\n",
    "            evaluation_prompt_template.format(original=row[col_name], translation=row[\"translation\"])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        scores = []\n",
    "        for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "            batch_prompts = prompts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=10,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            for full_output, prompt in zip(decoded, batch_prompts):\n",
    "                gen = full_output[len(prompt):].strip()\n",
    "                score = extract_score(gen)\n",
    "                scores.append(score)\n",
    "\n",
    "        df[\"score\"] = scores\n",
    "        output_path = f\"scored_qwen_{prompt_name}.csv\" if model_name == \"qwen:7B\" else f\"scored_{model_name}_{prompt_name}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Punteggi salvati in '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b366ffb",
   "metadata": {},
   "source": [
    "LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1.  **Completely Unacceptable Translation:**\n",
    "    *   The translation has no pertinence with the original meaning.\n",
    "    *   The generated sentence is either gibberish, makes no sense, or is completely unrelated to the archaic text.\n",
    "    *   Contains severe errors that render it incomprehensible or entirely misleading.\n",
    "\n",
    "2.  **Severe Semantic Errors/Omissions:**\n",
    "    *   The translation contains significant semantic errors, critical omissions of meaning from the archaic text, or substantial incorrect additions.\n",
    "    *   While some words might be recognizable, the core meaning is lost or heavily distorted.\n",
    "    *   The modernization is poor, leaving many archaic forms or incorrectly modernizing them.\n",
    "    *   Likely many grammatical errors in modern Italian.\n",
    "\n",
    "3.  **Partially Wrong Translation / Lackluster:**\n",
    "    *   The translation captures some of the original meaning but is lackluster or contains noticeable errors.\n",
    "    *   Errors are mostly minor (e.g., awkward phrasing, typos, minor grammatical mistakes in modern Italian, some less critical semantic misunderstandings or misinterpretations of archaic terms).\n",
    "    *   Some archaic features might be awkwardly modernized or missed.\n",
    "    *   The overall quality is mediocre; it's understandable but clearly flawed.\n",
    "\n",
    "4.  **Good Translation:**\n",
    "    *   The translation is mostly accurate and successfully conveys the core meaning of the archaic sentence.\n",
    "    *   It is substantially faithful to the original text.\n",
    "    *   The modern Italian is fluent and comprehensible.\n",
    "    *   Archaic features are generally well modernized.\n",
    "    *   There might be minor stylistic imperfections (e.g., style doesn't perfectly match natural modern Italian, slight awkwardness) or very minor errors that do not significantly impact understanding or meaning.\n",
    "\n",
    "5.  **Perfect Translation:**\n",
    "    *   The translation is completely accurate, fully conveying the meaning and nuances of the original archaic sentence.\n",
    "    *   It is perfectly fluent, natural-sounding, and grammatically correct modern Italian.\n",
    "    *   All archaic linguistic features (vocabulary, syntax, orthography) are correctly and appropriately modernized.\n",
    "    *   The style is appropriate for modern Italian.\n",
    "    *   No errors.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric Defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed498a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(archaic_text, modern_translation, rubric):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "                Your task is to assess the quality of the \"Candidate Modern Translation\" provided below, based on the \"Original Archaic Sentence\".\n",
    "\n",
    "                Please use the following detailed 1-5 scale and rubric for your evaluation:\n",
    "                --- RUBRIC START ---\n",
    "                {rubric}\n",
    "                --- RUBRIC END ---\n",
    "\n",
    "                Original Archaic Sentence:\n",
    "                \"{archaic_text}\"\n",
    "\n",
    "                Candidate Modern Translation:\n",
    "                \"{modern_translation}\"\n",
    "\n",
    "                Carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "                Output ONLY the integer score. Do not add any explanation, prefix, or other text.\n",
    "\n",
    "                Score:\"\"\"\n",
    "\n",
    "print(\"Judge Prompting Function Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re # For parsing the score\n",
    "\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "JUDGE_MODEL_NAME = \"gemini-pro\"\n",
    "\n",
    "\n",
    "\n",
    "llms = []\n",
    "prompts = []\n",
    "\n",
    "for llm in llms:\n",
    "\n",
    "    for prompt in prompts:\n",
    "\n",
    "        CSV_FILE_PATH = \"translations.csv\"\n",
    "        OUTPUT_CSV_PATH = \"judged_translations.csv\"\n",
    "\n",
    "\n",
    "        TRANSLATION_SYSTEM_COLUMNS = [\"translation_system_1\", \"translation_system_2\", \"translation_system_3\"]\n",
    "        ARCHAIC_SENTENCE_COLUMN = \"archaic_sentence\"\n",
    "\n",
    "        print(\"Setup Complete.\")\n",
    "        print(\"\\nJudge Model:\", JUDGE_MODEL_NAME)\n",
    "        print(\"\\nJudging: \",) # The model and prompt that is judging\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a09461",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_translations = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"Successfully loaded {len(df_translations)} rows from {CSV_FILE_PATH}\")\n",
    "    # Display first few rows to verify\n",
    "    print(\"\\nFirst 5 rows of your data:\")\n",
    "    print(df_translations.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file {CSV_FILE_PATH} was not found. Please check the path.\")\n",
    "    df_translations = pd.DataFrame() # Create empty df to avoid later errors if notebook run continues\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    df_translations = pd.DataFrame()\n",
    "\n",
    "# List to store all judgment results\n",
    "all_judgments_data = []\n",
    "\n",
    "\n",
    "# Initialize the generative model\n",
    "judge_model = genai.GenerativeModel(JUDGE_MODEL_NAME)\n",
    "print(f\"Using Judge Model: {judge_model.model_name}\")\n",
    "\n",
    "# Safety setting for generation - can be adjusted.\n",
    "# Higher values for 'block_...' mean more conservative blocking.\n",
    "# See https://ai.google.dev/docs/safety_setting_gemini\n",
    "# You might want to adjust these if your content is being blocked.\n",
    "# For this task, default should be mostly fine.\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    # temperature=0.1 # For more deterministic output from the judge\n",
    ")\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "\n",
    "if not df_translations.empty:\n",
    "    print(f\"\\nStarting judging process for {len(df_translations)} archaic sentences...\")\n",
    "    for index, row in df_translations.iterrows():\n",
    "        archaic_sent = str(row[ARCHAIC_SENTENCE_COLUMN]) # Ensure it's a string\n",
    "        print(f\"\\nJudging translations for archaic sentence {index + 1}/{len(df_translations)}: \\\"{archaic_sent[:70]}...\\\"\")\n",
    "\n",
    "        for system_col_name in TRANSLATION_SYSTEM_COLUMNS:\n",
    "            modern_trans = str(row[system_col_name]) if pd.notna(row[system_col_name]) else \"\" # Handle potential NaN/empty translations\n",
    "\n",
    "            if not modern_trans: # Skip if no translation from this system\n",
    "                print(f\"  - System '{system_col_name}': No translation provided. Skipping.\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"NO_TRANSLATION_PROVIDED\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": \"No translation provided by system\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            prompt_text = create_judge_prompt(archaic_sent, modern_trans, JUDGE_RUBRIC)\n",
    "\n",
    "            try:\n",
    "                print(f\"  - System '{system_col_name}': Sending to judge...\")\n",
    "                response = judge_model.generate_content(\n",
    "                    prompt_text,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings\n",
    "                )\n",
    "                raw_score_text = response.text.strip()\n",
    "                print(f\"    Raw response from Judge: '{raw_score_text}'\")\n",
    "\n",
    "                # Attempt to parse the score\n",
    "                parsed_score = None\n",
    "                error_msg = None\n",
    "                match = re.search(r'^\\s*([1-5])\\s*$', raw_score_text) # Looks for a single digit 1-5, allows whitespace\n",
    "                if match:\n",
    "                    parsed_score = int(match.group(1))\n",
    "                else:\n",
    "                    # Fallback if LLM didn't follow instructions perfectly\n",
    "                    fallback_match = re.search(r'\\b([1-5])\\b', raw_score_text) # Finds a digit 1-5 within other text\n",
    "                    if fallback_match:\n",
    "                        parsed_score = int(fallback_match.group(1))\n",
    "                        print(f\"    Warning: Parsed score '{parsed_score}' from less strict match. LLM Response: '{raw_score_text}'\")\n",
    "                    else:\n",
    "                        error_msg = f\"Could not parse a 1-5 score from: '{raw_score_text}'\"\n",
    "                        print(f\"    ERROR: {error_msg}\")\n",
    "\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": raw_score_text,\n",
    "                    \"judge_llm_score_parsed\": parsed_score,\n",
    "                    \"error_message\": error_msg\n",
    "                })\n",
    "\n",
    "            except genai.types.generation_types.BlockedPromptException as bpe:\n",
    "                print(f\"    ERROR: Prompt for '{system_col_name}' was blocked. Reason: {bpe}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"BLOCKED_PROMPT\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": f\"Prompt blocked by API: {bpe}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR judging translation from '{system_col_name}': {e}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"API_ERROR\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": str(e)\n",
    "                })\n",
    "\n",
    "            # Be mindful of API rate limits (requests per minute)\n",
    "            # Gemini free tier allows 60 RPM for gemini-pro.\n",
    "            # If you have many items or use a model with stricter limits, uncomment and adjust sleep time.\n",
    "            # time.sleep(1.1) # Sleep for a bit over 1 second to stay under 60 RPM\n",
    "\n",
    "    print(\"\\nJudging process complete.\")\n",
    "else:\n",
    "    print(\"No data loaded from CSV. Skipping judging process.\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_judged_results = pd.DataFrame(all_judgments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3096571",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_judged_results.empty:\n",
    "    print(\"\\n--- Judged Results (First 10 rows) ---\")\n",
    "    print(df_judged_results.head(10))\n",
    "\n",
    "    print(\"\\n--- Value Counts of Parsed Scores ---\")\n",
    "    print(df_judged_results['judge_llm_score_parsed'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    print(\"\\n--- Cases with Parsing Errors or API Issues ---\")\n",
    "    print(df_judged_results[df_judged_results['judge_llm_score_parsed'].isna() & (df_judged_results['error_message'] != \"No translation provided by system\")])\n",
    "\n",
    "\n",
    "    # Save the results to a new CSV\n",
    "    try:\n",
    "        df_judged_results.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "        print(f\"\\nSuccessfully saved judged results to {OUTPUT_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No judgments were made. Output DataFrame is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
