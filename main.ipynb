{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abe49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errori nelle traduzioni da fixare\n",
    "\"\"\"\n",
    "    gemma detailed : fornisce anche la spiegazione della risposta\n",
    "    llama3 tutti: fornisce anche la spiegazione nella risposta, ancora piu dettagliata in detailed and role_based\n",
    "\n",
    "\"\"\"\n",
    "#####\n",
    "# Note \n",
    "\"\"\"\n",
    "    Cerb detailed e role based fa una traduzione in inglese (scrivere nel report se persiste)\n",
    "    Cerb teacher stud alcune inglese e alcune non solo traduzione \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1c7d8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config success:\n",
      "Models: ['galatolo/cerbero-7b', 'gemma', 'llama3']\n",
      "Prompts: ['base', 'detailed', 'few_shot', 'role_based', 'teacher_student']\n",
      "Configuration loaded successfully.\n",
      "Models: ['galatolo/cerbero-7b', 'gemma', 'llama3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# open the config file \n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    models = config.get('models', [])\n",
    "    prompts = config.get('prompts', [])\n",
    "\n",
    "    print(\"Config success:\")\n",
    "    print(f\"Models: {models}\")\n",
    "    print(f\"Prompts: {prompts}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Config file '{CONFIG_FILE_PATH}' not found.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: File '{CONFIG_FILE_PATH}' not a valid JSON.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa64",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"dataset/dataset_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path) #creation of dataframe\n",
    "output_path = \"dataset/output.csv\"\n",
    "col_name = \"Sentence\"\n",
    "# models loaded from config file\n",
    "#prompts\n",
    "prompt_templates={\n",
    "    \"base\":        \"Translate the following sentence from archaic italian to modern italian:\\n\\n{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"detailed\": \"The following text is written in archaic Italian from the 13th century, originating from the Tuscany region. Rewrite it in modern Italian while preserving the original meaning, clarity, and syntactic coherence. First analyze the structure, then identify key words, then write the final version:\\n\\n {sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"role-based\":   \"You are an expert linguist specializing in the evolution of Italian language. Translate this 13th century Tuscan text to contemporary Italian while preserving the original meaning, clarity, and syntactic coherence: \\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"few_shot\": (\n",
    "            \"Here are some examples of sentences in archaic Italian from the 13th century translated into modern Italian:\\n\\n\"\n",
    "            \"Archaic Italian: «quella guerra ben fatta l' opera perché etc.». Modern Italian: «quella guerra fu condotta bene, e l'opera fu compiuta come previsto.».\\n\"\n",
    "            \"Archaic Italian: «crudele, e di tutte le colpe pigli vendetta». Modern Italian: «crudele, e si vendica di tutte le colpe.»\\n\"\n",
    "            \"Archaic Italian: «Non d' altra forza d' animo fue ornato Ponzio Aufidiano». Modern Italian: «Ponzio Aufidiano non era dotato di altro vigore d’animo.»\\n\\n\"\n",
    "            \"Now translate the following sentence from archaic italian to modern italian while preserving the original meaning:\\n\\n\"\n",
    "            \"{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\"\n",
    "                ),\n",
    "    \"teacher_student\": (\n",
    "            \"A student asked: 'What does this old Italian sentence mean in modern language?'\\n\"\n",
    "            \"You, a university professor of historical linguistics, respond with a clear and faithful modern Italian translation\\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb4dde",
   "metadata": {},
   "source": [
    "## Translate the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')#client to local ollama\n",
    "\n",
    "for model_name in models:#iterate through models\n",
    "    for prompt_name, prompt_template in prompt_templates.items():#iterate through prompt templates\n",
    "        print(f\"\\n Translation with model: {model_name} | prompt: {prompt_name}\")\n",
    "        translations = []#list to store translations\n",
    "        for sentence in tqdm(df[col_name]):#iterate through sentences\n",
    "            try:\n",
    "                prompt = prompt_template.format(sentence=sentence)#give the prompt\n",
    "\n",
    "                response = client.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )#ollama api call\n",
    "\n",
    "                translation = response['message']['content'].strip()#extract the translation\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                translation = f\"[ERROR]: {e}\"\n",
    "            translations.append(translation)#append the translation to the list\n",
    "\n",
    "        df[\"translation\"] = translations# add the translations to the dataframe\n",
    "        # TODO\n",
    "        if model_name == \"qwen:7B\":\n",
    "            output_file = f\"translations/translation_qwen_{prompt_name}.csv\"# create the output file name\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            output_file = f\"translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            output_file = f\"translations/translation_{model_name}_{prompt_name}.csv\"# create the output file name\n",
    "\n",
    "        df.to_csv(output_file, index=False)# save the dataframe to a csv file\n",
    "        print(f\"Translation saved in '{output_file}'\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015fd61",
   "metadata": {},
   "source": [
    "## Evaluation with Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3447948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "cache_dir = \"./models/m_prometheus_3b\"  #local directory to cache the model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf1bc7",
   "metadata": {},
   "source": [
    "### Evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b475dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "\n",
    "Original Archaic Sentence:\n",
    "{original}\n",
    "\n",
    "Candidate Modern Translation:\n",
    "{translation}\n",
    "\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1. **Completely Unacceptable Translation:** Meaningless or totally wrong.\n",
    "2. **Severe Semantic Errors/Omissions:** Big meaning problems, serious issues.\n",
    "3. **Partially Wrong Translation / Lackluster:** Understandable but flawed.\n",
    "4. **Good Translation:** Mostly correct, minor issues.\n",
    "5. **Perfect Translation:** Fully correct, fluent, natural.\n",
    "\n",
    "Based on the above criteria, respond only with the numeric score (1 to 5). Do not explain or write anything else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b40e0f",
   "metadata": {},
   "source": [
    "### Give the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import TextGenerationPipeline \n",
    "judge_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)# Use GPU if available, otherwise CPU\n",
    "\n",
    "# Function to extract score from the model's output\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        score = int(output_text.strip().split()[0])\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "    except:\n",
    "        pass\n",
    "    return \"[INVALID OUTPUT]\"\n",
    "\n",
    "for model_name in models:# iterate through translations\n",
    "    for prompt_name in prompt_templates:\n",
    "        # TODO\n",
    "\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"translations/translation_qwen_{prompt_name}.csv\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            file_path = f\"translations/translation_{model_name}_{prompt_name}.csv\"\n",
    "        df_translation = pd.read_csv(file_path)\n",
    "\n",
    "        scores = []# list to store scores\n",
    "        for _, row in tqdm(df_translation.iterrows(), total=len(df_translation)):\n",
    "            original = row[col_name]\n",
    "            translation = row[\"translation\"]\n",
    "\n",
    "            prompt = evaluation_prompt_template.format(original = original, translation=translation)# create the prompt for evaluation\n",
    "\n",
    "            output = judge_pipeline(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]# generate the output using the model\n",
    "            score = extract_score(output[len(prompt):])  # extract the score from the output\n",
    "            scores.append(score)# append the score to the list\n",
    "\n",
    "        df_translation[\"Score\"] = scores# add the scores to the dataframe\n",
    "        # TODO\n",
    "        #save the dataframe with scores\n",
    "        if model_name == \"qwen:7B\":\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_qwen_{prompt_name}.csv\", index=False)\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_cerbero_{prompt_name}.csv\", index=False)\n",
    "        else:\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_{model_name}_{prompt_name}.csv\", index=False)\n",
    "        print(f\"PScores saved in 'scored_{model_name}_{prompt_name}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff4c02",
   "metadata": {},
   "source": [
    "### Average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for model_name in models:#iterate through scores\n",
    "    for prompt_name in prompt_templates.keys():\n",
    "        # TODO\n",
    "\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"scores_prometheus/scored_qwen_{prompt_name}.csv\"\n",
    "            short_model = \"qwen\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"scores_prometheus/scored_cerbero_{prompt_name}.csv\"\n",
    "            short_model = \"cerbero\"\n",
    "        else:\n",
    "            file_path = f\"scores_prometheus/scored_{model_name}_{prompt_name}.csv\"\n",
    "            short_model = model_name.replace(\"/\", \"\").replace(\":\", \"_\")\n",
    "\n",
    "        # Check if the file exists and read it\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if \"score\" in df.columns:\n",
    "                mean_score = df[\"score\"].mean()# calculate the mean score\n",
    "                results.append({\n",
    "                    \"model\": short_model,\n",
    "                    \"prompt\": prompt_name,\n",
    "                    \"mean_score\": mean_score\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Column'score' not found in {file_path}\")\n",
    "        else:\n",
    "            print(f\"file not found in {file_path}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)# create a dataframe with the results\n",
    "\n",
    "os.makedirs(\"scores\", exist_ok=True)\n",
    "df_results.to_csv(\"final_scores_prometheus.csv\", index=False)# save the results to a csv file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
