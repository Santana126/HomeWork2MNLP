{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abe49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errori nelle traduzioni da fixare\n",
    "\"\"\"\n",
    "    gemma detailed : fornisce anche la spiegazione della risposta\n",
    "    llama3 tutti: fornisce anche la spiegazione nella risposta, ancora piu dettagliata in detailed and role_based\n",
    "\n",
    "\"\"\"\n",
    "#####\n",
    "# Note \n",
    "\"\"\"\n",
    "    Cerb detailed e role based fa una traduzione in inglese (scrivere nel report se persiste)\n",
    "    Cerb teacher stud alcune inglese e alcune non solo traduzione \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1c7d8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config success:\n",
      "Models: ['galatolo/cerbero-7b', 'gemma', 'llama3']\n",
      "Prompts: ['base', 'detailed', 'few_shot', 'role_based', 'teacher_student']\n",
      "Configuration loaded successfully.\n",
      "Models: ['galatolo/cerbero-7b', 'gemma', 'llama3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# open the config file \n",
    "CONFIG_FILE_PATH = \"config.json\"\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    models = config.get('models', [])\n",
    "    prompts = config.get('prompts', [])\n",
    "\n",
    "    print(\"Config success:\")\n",
    "    print(f\"Models: {models}\")\n",
    "    print(f\"Prompts: {prompts}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Config file '{CONFIG_FILE_PATH}' not found.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: File '{CONFIG_FILE_PATH}' not a valid JSON.\")\n",
    "    models = []\n",
    "    prompts = []\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa64",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = \"dataset/dataset_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path) #creation of dataframe\n",
    "output_path = \"dataset/output.csv\"\n",
    "col_name = \"Sentence\"\n",
    "# models loaded from config file\n",
    "#prompts\n",
    "prompt_templates={\n",
    "    \"base\":        \"Translate the following sentence from archaic italian to modern italian:\\n\\n{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"detailed\": \"The following text is written in archaic Italian from the 13th century, originating from the Tuscany region. Rewrite it in modern Italian while preserving the original meaning, clarity, and syntactic coherence. First analyze the structure, then identify key words, then write the final version:\\n\\n {sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"role-based\":   \"You are an expert linguist specializing in the evolution of Italian language. Translate this 13th century Tuscan text to contemporary Italian while preserving the original meaning, clarity, and syntactic coherence: \\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\",\n",
    "    \"few_shot\": (\n",
    "            \"Here are some examples of sentences in archaic Italian from the 13th century translated into modern Italian:\\n\\n\"\n",
    "            \"Archaic Italian: «quella guerra ben fatta l' opera perché etc.». Modern Italian: «quella guerra fu condotta bene, e l'opera fu compiuta come previsto.».\\n\"\n",
    "            \"Archaic Italian: «crudele, e di tutte le colpe pigli vendetta». Modern Italian: «crudele, e si vendica di tutte le colpe.»\\n\"\n",
    "            \"Archaic Italian: «Non d' altra forza d' animo fue ornato Ponzio Aufidiano». Modern Italian: «Ponzio Aufidiano non era dotato di altro vigore d’animo.»\\n\\n\"\n",
    "            \"Now translate the following sentence from archaic italian to modern italian while preserving the original meaning:\\n\\n\"\n",
    "            \"{sentence}\\n\\n.The output must be ONLY the translated sentence in modern italian:\"\n",
    "                ),\n",
    "    \"teacher_student\": (\n",
    "            \"A student asked: 'What does this old Italian sentence mean in modern language?'\\n\"\n",
    "            \"You, a university professor of historical linguistics, respond with a clear and faithful modern Italian translation\\n\\n{sentence}\\n\\nThe output must be ONLY the translated sentence in modern italian:\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb4dde",
   "metadata": {},
   "source": [
    "## Translate the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')#client to local ollama\n",
    "\n",
    "for model_name in models:#iterate through models\n",
    "    for prompt_name, prompt_template in prompt_templates.items():#iterate through prompt templates\n",
    "        print(f\"\\n Translation with model: {model_name} | prompt: {prompt_name}\")\n",
    "        translations = []#list to store translations\n",
    "        for sentence in tqdm(df[col_name]):#iterate through sentences\n",
    "            try:\n",
    "                prompt = prompt_template.format(sentence=sentence)#give the prompt\n",
    "\n",
    "                response = client.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )#ollama api call\n",
    "\n",
    "                translation = response['message']['content'].strip()#extract the translation\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                translation = f\"[ERROR]: {e}\"\n",
    "            translations.append(translation)#append the translation to the list\n",
    "\n",
    "        df[\"translation\"] = translations# add the translations to the dataframe\n",
    "        # TODO\n",
    "        if model_name == \"qwen:7B\":\n",
    "            output_file = f\"translations/translation_qwen_{prompt_name}.csv\"# create the output file name\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            output_file = f\"translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            output_file = f\"translations/translation_{model_name}_{prompt_name}.csv\"# create the output file name\n",
    "\n",
    "        df.to_csv(output_file, index=False)# save the dataframe to a csv file\n",
    "        print(f\"Translation saved in '{output_file}'\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015fd61",
   "metadata": {},
   "source": [
    "## Evaluation with Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "cache_dir = \"./models/m_prometheus_3b\"  #local directory to cache the model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf1bc7",
   "metadata": {},
   "source": [
    "### Evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "\n",
    "Original Archaic Sentence:\n",
    "{original}\n",
    "\n",
    "Candidate Modern Translation:\n",
    "{translation}\n",
    "\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1. **Completely Unacceptable Translation:** Meaningless or totally wrong.\n",
    "2. **Severe Semantic Errors/Omissions:** Big meaning problems, serious issues.\n",
    "3. **Partially Wrong Translation / Lackluster:** Understandable but flawed.\n",
    "4. **Good Translation:** Mostly correct, minor issues.\n",
    "5. **Perfect Translation:** Fully correct, fluent, natural.\n",
    "\n",
    "Based on the above criteria, respond only with the numeric score (1 to 5). Do not explain or write anything else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b40e0f",
   "metadata": {},
   "source": [
    "### Give the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import TextGenerationPipeline \n",
    "judge_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)# # Use GPU if available, otherwise CPU\n",
    "\n",
    "# Function to extract score from the model's output\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        score = int(output_text.strip().split()[0])\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "    except:\n",
    "        pass\n",
    "    return \"[INVALID OUTPUT]\"\n",
    "\n",
    "for model_name in models:# iterate through translations\n",
    "    for prompt_name in prompt_templates:\n",
    "        # TODO\n",
    "\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"translations/translation_qwen_{prompt_name}.csv\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"translations/translation_cerbero_{prompt_name}.csv\"\n",
    "        else:\n",
    "            file_path = f\"translations/translation_{model_name}_{prompt_name}.csv\"\n",
    "        df_translation = pd.read_csv(file_path)\n",
    "\n",
    "        scores = []# list to store scores\n",
    "        for _, row in tqdm(df_translation.iterrows(), total=len(df_translation)):\n",
    "            original = row[col_name]\n",
    "            translation = row[\"translation\"]\n",
    "\n",
    "            prompt = evaluation_prompt_template.format(original = original, translation=translation)# create the prompt for evaluation\n",
    "\n",
    "            output = judge_pipeline(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]# generate the output using the model\n",
    "            score = extract_score(output[len(prompt):])  # extract the score from the output\n",
    "            scores.append(score)# append the score to the list\n",
    "\n",
    "        df_translation[\"score\"] = scores# add the scores to the dataframe\n",
    "        # TODO\n",
    "        #save the dataframe with scores\n",
    "        if model_name == \"qwen:7B\":\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_qwen_{prompt_name}.csv\", index=False)\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_cerbero_{prompt_name}.csv\", index=False)\n",
    "        else:\n",
    "            df_translation.to_csv(f\"scores_prometheus/scored_{model_name}_{prompt_name}.csv\", index=False)\n",
    "        print(f\"PScores saved in 'scored_{model_name}_{prompt_name}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff4c02",
   "metadata": {},
   "source": [
    "### Average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for model_name in models:#iterate through scores\n",
    "    for prompt_name in prompt_templates.keys():\n",
    "        # TODO\n",
    "\n",
    "        if model_name == \"qwen:7B\":\n",
    "            file_path = f\"scores_prometheus/scored_qwen_{prompt_name}.csv\"\n",
    "            short_model = \"qwen\"\n",
    "        elif model_name == \"galatolo/cerbero-7b\":\n",
    "            file_path = f\"scores_prometheus/scored_cerbero_{prompt_name}.csv\"\n",
    "            short_model = \"cerbero\"\n",
    "        else:\n",
    "            file_path = f\"scores_prometheus/scored_{model_name}_{prompt_name}.csv\"\n",
    "            short_model = model_name.replace(\"/\", \"\").replace(\":\", \"_\")\n",
    "\n",
    "        # Check if the file exists and read it\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if \"score\" in df.columns:\n",
    "                mean_score = df[\"score\"].mean()# calculate the mean score\n",
    "                results.append({\n",
    "                    \"model\": short_model,\n",
    "                    \"prompt\": prompt_name,\n",
    "                    \"mean_score\": mean_score\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Column'score' not found in {file_path}\")\n",
    "        else:\n",
    "            print(f\"file not found in {file_path}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)# create a dataframe with the results\n",
    "\n",
    "os.makedirs(\"scores\", exist_ok=True)\n",
    "df_results.to_csv(\"final_scores_prometheus.csv\", index=False)# save the results to a csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecae8f",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe12ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # TODO check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b366ffb",
   "metadata": {},
   "source": [
    "LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating the quality of a modern Italian translation based on an original archaic Italian sentence.\n",
    "Please score the \"Candidate Modern Translation\" using the following 1-5 scale:\n",
    "\n",
    "1.  **Completely Unacceptable Translation:**\n",
    "    *   The translation has no pertinence with the original meaning.\n",
    "    *   The generated sentence is either gibberish, makes no sense, or is completely unrelated to the archaic text.\n",
    "    *   Contains severe errors that render it incomprehensible or entirely misleading.\n",
    "\n",
    "2.  **Severe Semantic Errors/Omissions:**\n",
    "    *   The translation contains significant semantic errors, critical omissions of meaning from the archaic text, or substantial incorrect additions.\n",
    "    *   While some words might be recognizable, the core meaning is lost or heavily distorted.\n",
    "    *   The modernization is poor, leaving many archaic forms or incorrectly modernizing them.\n",
    "    *   Likely many grammatical errors in modern Italian.\n",
    "\n",
    "3.  **Partially Wrong Translation / Lackluster:**\n",
    "    *   The translation captures some of the original meaning but is lackluster or contains noticeable errors.\n",
    "    *   Errors are mostly minor (e.g., awkward phrasing, typos, minor grammatical mistakes in modern Italian, some less critical semantic misunderstandings or misinterpretations of archaic terms).\n",
    "    *   Some archaic features might be awkwardly modernized or missed.\n",
    "    *   The overall quality is mediocre; it's understandable but clearly flawed.\n",
    "\n",
    "4.  **Good Translation:**\n",
    "    *   The translation is mostly accurate and successfully conveys the core meaning of the archaic sentence.\n",
    "    *   It is substantially faithful to the original text.\n",
    "    *   The modern Italian is fluent and comprehensible.\n",
    "    *   Archaic features are generally well modernized.\n",
    "    *   There might be minor stylistic imperfections (e.g., style doesn't perfectly match natural modern Italian, slight awkwardness) or very minor errors that do not significantly impact understanding or meaning.\n",
    "\n",
    "5.  **Perfect Translation:**\n",
    "    *   The translation is completely accurate, fully conveying the meaning and nuances of the original archaic sentence.\n",
    "    *   It is perfectly fluent, natural-sounding, and grammatically correct modern Italian.\n",
    "    *   All archaic linguistic features (vocabulary, syntax, orthography) are correctly and appropriately modernized.\n",
    "    *   The style is appropriate for modern Italian.\n",
    "    *   No errors.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Rubric Defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed498a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(archaic_text, modern_translation, rubric):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an expert evaluator specializing in the translation of archaic Italian to modern Italian.\n",
    "                Your task is to assess the quality of the \"Candidate Modern Translation\" provided below, based on the \"Original Archaic Sentence\".\n",
    "\n",
    "                Please use the following detailed 1-5 scale and rubric for your evaluation:\n",
    "                --- RUBRIC START ---\n",
    "                {rubric}\n",
    "                --- RUBRIC END ---\n",
    "\n",
    "                Original Archaic Sentence:\n",
    "                \"{archaic_text}\"\n",
    "\n",
    "                Candidate Modern Translation:\n",
    "                \"{modern_translation}\"\n",
    "\n",
    "                Carefully consider the rubric. Based on your assessment, provide a single integer score from 1 to 5 that best reflects the quality of the \"Candidate Modern Translation\".\n",
    "                Output ONLY the integer score. Do not add any explanation, prefix, or other text.\n",
    "\n",
    "                Score:\"\"\"\n",
    "\n",
    "print(\"Judge Prompting Function Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8722c9f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re # For parsing the score\n",
    "\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "JUDGE_MODEL_NAME = \"gemini-pro\"\n",
    "\n",
    "\n",
    "\n",
    "llms = []\n",
    "prompts = []\n",
    "\n",
    "for llm in llms:\n",
    "\n",
    "    for prompt in prompts:\n",
    "\n",
    "        CSV_FILE_PATH = \"translations.csv\"\n",
    "        OUTPUT_CSV_PATH = \"judged_translations.csv\"\n",
    "\n",
    "\n",
    "        TRANSLATION_SYSTEM_COLUMNS = [\"translation_system_1\", \"translation_system_2\", \"translation_system_3\"]\n",
    "        ARCHAIC_SENTENCE_COLUMN = \"archaic_sentence\"\n",
    "\n",
    "        print(\"Setup Complete.\")\n",
    "        print(\"\\nJudge Model:\", JUDGE_MODEL_NAME)\n",
    "        print(\"\\nJudging: \",) # The model and prompt that is judging\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a09461",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_translations = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"Successfully loaded {len(df_translations)} rows from {CSV_FILE_PATH}\")\n",
    "    # Display first few rows to verify\n",
    "    print(\"\\nFirst 5 rows of your data:\")\n",
    "    print(df_translations.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file {CSV_FILE_PATH} was not found. Please check the path.\")\n",
    "    df_translations = pd.DataFrame() # Create empty df to avoid later errors if notebook run continues\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    df_translations = pd.DataFrame()\n",
    "\n",
    "# List to store all judgment results\n",
    "all_judgments_data = []\n",
    "\n",
    "\n",
    "# Initialize the generative model\n",
    "judge_model = genai.GenerativeModel(JUDGE_MODEL_NAME)\n",
    "print(f\"Using Judge Model: {judge_model.model_name}\")\n",
    "\n",
    "# Safety setting for generation - can be adjusted.\n",
    "# Higher values for 'block_...' mean more conservative blocking.\n",
    "# See https://ai.google.dev/docs/safety_setting_gemini\n",
    "# You might want to adjust these if your content is being blocked.\n",
    "# For this task, default should be mostly fine.\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    # temperature=0.1 # For more deterministic output from the judge\n",
    ")\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "\n",
    "if not df_translations.empty:\n",
    "    print(f\"\\nStarting judging process for {len(df_translations)} archaic sentences...\")\n",
    "    for index, row in df_translations.iterrows():\n",
    "        archaic_sent = str(row[ARCHAIC_SENTENCE_COLUMN]) # Ensure it's a string\n",
    "        print(f\"\\nJudging translations for archaic sentence {index + 1}/{len(df_translations)}: \\\"{archaic_sent[:70]}...\\\"\")\n",
    "\n",
    "        for system_col_name in TRANSLATION_SYSTEM_COLUMNS:\n",
    "            modern_trans = str(row[system_col_name]) if pd.notna(row[system_col_name]) else \"\" # Handle potential NaN/empty translations\n",
    "\n",
    "            if not modern_trans: # Skip if no translation from this system\n",
    "                print(f\"  - System '{system_col_name}': No translation provided. Skipping.\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"NO_TRANSLATION_PROVIDED\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": \"No translation provided by system\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            prompt_text = create_judge_prompt(archaic_sent, modern_trans, JUDGE_RUBRIC)\n",
    "\n",
    "            try:\n",
    "                print(f\"  - System '{system_col_name}': Sending to judge...\")\n",
    "                response = judge_model.generate_content(\n",
    "                    prompt_text,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings\n",
    "                )\n",
    "                raw_score_text = response.text.strip()\n",
    "                print(f\"    Raw response from Judge: '{raw_score_text}'\")\n",
    "\n",
    "                # Attempt to parse the score\n",
    "                parsed_score = None\n",
    "                error_msg = None\n",
    "                match = re.search(r'^\\s*([1-5])\\s*$', raw_score_text) # Looks for a single digit 1-5, allows whitespace\n",
    "                if match:\n",
    "                    parsed_score = int(match.group(1))\n",
    "                else:\n",
    "                    # Fallback if LLM didn't follow instructions perfectly\n",
    "                    fallback_match = re.search(r'\\b([1-5])\\b', raw_score_text) # Finds a digit 1-5 within other text\n",
    "                    if fallback_match:\n",
    "                        parsed_score = int(fallback_match.group(1))\n",
    "                        print(f\"    Warning: Parsed score '{parsed_score}' from less strict match. LLM Response: '{raw_score_text}'\")\n",
    "                    else:\n",
    "                        error_msg = f\"Could not parse a 1-5 score from: '{raw_score_text}'\"\n",
    "                        print(f\"    ERROR: {error_msg}\")\n",
    "\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": raw_score_text,\n",
    "                    \"judge_llm_score_parsed\": parsed_score,\n",
    "                    \"error_message\": error_msg\n",
    "                })\n",
    "\n",
    "            except genai.types.generation_types.BlockedPromptException as bpe:\n",
    "                print(f\"    ERROR: Prompt for '{system_col_name}' was blocked. Reason: {bpe}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"BLOCKED_PROMPT\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": f\"Prompt blocked by API: {bpe}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR judging translation from '{system_col_name}': {e}\")\n",
    "                all_judgments_data.append({\n",
    "                    \"archaic_sentence\": archaic_sent,\n",
    "                    \"translation_system\": system_col_name,\n",
    "                    \"modern_translation\": modern_trans,\n",
    "                    \"judge_llm_raw_output\": \"API_ERROR\",\n",
    "                    \"judge_llm_score_parsed\": None,\n",
    "                    \"error_message\": str(e)\n",
    "                })\n",
    "\n",
    "            # Be mindful of API rate limits (requests per minute)\n",
    "            # Gemini free tier allows 60 RPM for gemini-pro.\n",
    "            # If you have many items or use a model with stricter limits, uncomment and adjust sleep time.\n",
    "            # time.sleep(1.1) # Sleep for a bit over 1 second to stay under 60 RPM\n",
    "\n",
    "    print(\"\\nJudging process complete.\")\n",
    "else:\n",
    "    print(\"No data loaded from CSV. Skipping judging process.\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_judged_results = pd.DataFrame(all_judgments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3096571",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_judged_results.empty:\n",
    "    print(\"\\n--- Judged Results (First 10 rows) ---\")\n",
    "    print(df_judged_results.head(10))\n",
    "\n",
    "    print(\"\\n--- Value Counts of Parsed Scores ---\")\n",
    "    print(df_judged_results['judge_llm_score_parsed'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    print(\"\\n--- Cases with Parsing Errors or API Issues ---\")\n",
    "    print(df_judged_results[df_judged_results['judge_llm_score_parsed'].isna() & (df_judged_results['error_message'] != \"No translation provided by system\")])\n",
    "\n",
    "\n",
    "    # Save the results to a new CSV\n",
    "    try:\n",
    "        df_judged_results.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "        print(f\"\\nSuccessfully saved judged results to {OUTPUT_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No judgments were made. Output DataFrame is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
